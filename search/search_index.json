{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the WebMEV documentation page. This will serve as the main source of information and documentation on the architecture and structure of MEV and its RESTful API. For documentation on the endpoints, checkout the API specification","title":"Home"},{"location":"#welcome-to-the-webmev-documentation-page","text":"This will serve as the main source of information and documentation on the architecture and structure of MEV and its RESTful API. For documentation on the endpoints, checkout the API specification","title":"Welcome to the WebMEV documentation page."},{"location":"api/","text":"Documentation on the API There are a couple aspects to the WevMEV REST API. We have the actual API endpoints which are used to drive analysis or a frontend visualization. Documentation of the API interaction is provided by auto-generated documentation conforming to the OpenAPI spec here: API documentation Behind the API itself are the data structures and models that we use to architect the system. You will find information about these entities and their relationships in this section.","title":"Intro"},{"location":"api/#documentation-on-the-api","text":"There are a couple aspects to the WevMEV REST API. We have the actual API endpoints which are used to drive analysis or a frontend visualization. Documentation of the API interaction is provided by auto-generated documentation conforming to the OpenAPI spec here: API documentation Behind the API itself are the data structures and models that we use to architect the system. You will find information about these entities and their relationships in this section.","title":"Documentation on the API"},{"location":"attributes/","text":"Attributes Attribute s could also be thought of as \"parameters\" and are a way of providing validated key-value pairs. The different types enforce various constraints on the underlying primitive type (e.g. a float bounded between [0,1] can represent a probability). Mainly, Attribute s are a way to add information to Observation or Feature instances. For example, one could specify the phenotype or experimental group of an Observation via a StringAttribute . class api.data_structures.attributes. BaseAttribute ( value , **kwargs ) Base object which defines some common methods and members for Attribute types Classes that derive from BaseAttribute have strings which identify their type ( typename ) and a value , which is specific to the child class implementation. See child classes for examples. class api.data_structures.attributes. BoundedBaseAttribute ( value , **kwargs ) This class derives from BaseAttribute and adds logic for numeric attributes that are bounded between specified values. In addition to the typename and value members, these require a min and a max to set the bounds. Classes deriving from this can be used for things like bounding a p-value from a hypothesis test (which is 0<=p<=1) class api.data_structures.attributes. IntegerAttribute ( value , **kwargs ) General, unbounded integers. Represented by { \"attribute_type\": \"Integer\", \"value\": <integer> } class api.data_structures.attributes. PositiveIntegerAttribute ( value , **kwargs ) Integers > 0 { \"attribute_type\": \"PositiveInteger\", \"value\": <integer> } class api.data_structures.attributes. NonnegativeIntegerAttribute ( value , **kwargs ) Integers >=0 { \"attribute_type\": \"NonNegativeInteger\", \"value\": <integer> } class api.data_structures.attributes. BoundedIntegerAttribute ( value , **kwargs ) Integers that are bounded between a min and max value. { \"attribute_type\": \"BoundedInteger\", \"value\": <integer>, \"min\": <integer lower bound>, \"max\": <integer upper bound> } class api.data_structures.attributes. FloatAttribute ( value , **kwargs ) General, unbounded float type { \"attribute_type\": \"Float\", \"value\": <float> } class api.data_structures.attributes. PositiveFloatAttribute ( value , **kwargs ) Positive (>0) float type { \"attribute_type\": \"PositiveFloat\", \"value\": <float> } class api.data_structures.attributes. NonnegativeFloatAttribute ( value , **kwargs ) Non-negative (>=0) float type { \"attribute_type\": \"NonNegativeFloat\", \"value\": <float> } class api.data_structures.attributes. BoundedFloatAttribute ( value , **kwargs ) Floats that are bounded between a min and max value. { \"attribute_type\": \"BoundedFloat\", \"value\": <float>, \"min\": <integer/float lower bound>, \"max\": <integer/float upper bound> } class api.data_structures.attributes. StringAttribute ( value , **kwargs ) String type that has basic guards against non-typical characters. { \"attribute_type\": \"String\", \"value\": <str> } api.data_structures.attributes. create_attribute ( attr_key , attribute_dict ) Utility function used by the serializers to create/return BaseAttribute-derived instances. Accepts an attribute_dict which is a Python dictionary object containing the keys appropriate to create a particular attribute. For example, to create a BoundedIntegerAttribute , this dict would be formatted as, attr_dict = { 'attribute_type': 'BoundedInteger', 'value': 3, 'min': 0, 'max': 10 }","title":"Attributes"},{"location":"attributes/#attributes","text":"Attribute s could also be thought of as \"parameters\" and are a way of providing validated key-value pairs. The different types enforce various constraints on the underlying primitive type (e.g. a float bounded between [0,1] can represent a probability). Mainly, Attribute s are a way to add information to Observation or Feature instances. For example, one could specify the phenotype or experimental group of an Observation via a StringAttribute . class api.data_structures.attributes. BaseAttribute ( value , **kwargs ) Base object which defines some common methods and members for Attribute types Classes that derive from BaseAttribute have strings which identify their type ( typename ) and a value , which is specific to the child class implementation. See child classes for examples. class api.data_structures.attributes. BoundedBaseAttribute ( value , **kwargs ) This class derives from BaseAttribute and adds logic for numeric attributes that are bounded between specified values. In addition to the typename and value members, these require a min and a max to set the bounds. Classes deriving from this can be used for things like bounding a p-value from a hypothesis test (which is 0<=p<=1) class api.data_structures.attributes. IntegerAttribute ( value , **kwargs ) General, unbounded integers. Represented by { \"attribute_type\": \"Integer\", \"value\": <integer> } class api.data_structures.attributes. PositiveIntegerAttribute ( value , **kwargs ) Integers > 0 { \"attribute_type\": \"PositiveInteger\", \"value\": <integer> } class api.data_structures.attributes. NonnegativeIntegerAttribute ( value , **kwargs ) Integers >=0 { \"attribute_type\": \"NonNegativeInteger\", \"value\": <integer> } class api.data_structures.attributes. BoundedIntegerAttribute ( value , **kwargs ) Integers that are bounded between a min and max value. { \"attribute_type\": \"BoundedInteger\", \"value\": <integer>, \"min\": <integer lower bound>, \"max\": <integer upper bound> } class api.data_structures.attributes. FloatAttribute ( value , **kwargs ) General, unbounded float type { \"attribute_type\": \"Float\", \"value\": <float> } class api.data_structures.attributes. PositiveFloatAttribute ( value , **kwargs ) Positive (>0) float type { \"attribute_type\": \"PositiveFloat\", \"value\": <float> } class api.data_structures.attributes. NonnegativeFloatAttribute ( value , **kwargs ) Non-negative (>=0) float type { \"attribute_type\": \"NonNegativeFloat\", \"value\": <float> } class api.data_structures.attributes. BoundedFloatAttribute ( value , **kwargs ) Floats that are bounded between a min and max value. { \"attribute_type\": \"BoundedFloat\", \"value\": <float>, \"min\": <integer/float lower bound>, \"max\": <integer/float upper bound> } class api.data_structures.attributes. StringAttribute ( value , **kwargs ) String type that has basic guards against non-typical characters. { \"attribute_type\": \"String\", \"value\": <str> } api.data_structures.attributes. create_attribute ( attr_key , attribute_dict ) Utility function used by the serializers to create/return BaseAttribute-derived instances. Accepts an attribute_dict which is a Python dictionary object containing the keys appropriate to create a particular attribute. For example, to create a BoundedIntegerAttribute , this dict would be formatted as, attr_dict = { 'attribute_type': 'BoundedInteger', 'value': 3, 'min': 0, 'max': 10 }","title":"Attributes"},{"location":"auth/","text":"Authentication with MEV Once a user is registered (with an email and password), requests to the API are controlled with a JWT contained in the request header. Below is an example using Python's Requests library. This example assumes you have created a user. First, exchange the username/password to get the API token: import requests token_url = 'http://127.0.0.1:8000/api/token/' payload = {'email': '<EMAIL>', 'password': '<PASSWD>'} token_response = requests.post(token_url, data=payload) token_json = token_response.json() Then, looking at token_json : {'refresh': '<REFRESH TOKEN>', 'access': '<ACCESS_TOKEN>'} We can then use that token in requests to the API: access_token = token_json['access'] resource_list_url = 'http://127.0.0.1:8000/api/resources/' headers = {'Authorization': 'Bearer %s' % access_token} resource_response = requests.get(resource_list_url, headers=headers) resource_json = resource_response.json() If the token expires (a 401 response), you need to request a new token or refresh: refresh_url = 'http://127.0.0.1:8000/api/token/refresh/' payload = {'refresh': refresh_token} refresh_response = requests.post(refresh_url, data=payload) access_token = refresh_response.json()['access']","title":"Authentication"},{"location":"auth/#authentication-with-mev","text":"Once a user is registered (with an email and password), requests to the API are controlled with a JWT contained in the request header. Below is an example using Python's Requests library. This example assumes you have created a user. First, exchange the username/password to get the API token: import requests token_url = 'http://127.0.0.1:8000/api/token/' payload = {'email': '<EMAIL>', 'password': '<PASSWD>'} token_response = requests.post(token_url, data=payload) token_json = token_response.json() Then, looking at token_json : {'refresh': '<REFRESH TOKEN>', 'access': '<ACCESS_TOKEN>'} We can then use that token in requests to the API: access_token = token_json['access'] resource_list_url = 'http://127.0.0.1:8000/api/resources/' headers = {'Authorization': 'Bearer %s' % access_token} resource_response = requests.get(resource_list_url, headers=headers) resource_json = resource_response.json() If the token expires (a 401 response), you need to request a new token or refresh: refresh_url = 'http://127.0.0.1:8000/api/token/refresh/' payload = {'refresh': refresh_token} refresh_response = requests.post(refresh_url, data=payload) access_token = refresh_response.json()['access']","title":"Authentication with MEV"},{"location":"elements/","text":"Elements, Observations, and Features We adopt the convention from statistical learning of referring to Observation s and Feature s of data. Both of these data structures derive from BaseElement , which captures their common structure. Specialization specific to each can be overridden in the child classes. In an experimental context, Observation s are analogous to samples. That is, each Observation has one or more Feature s associated with it (e.g. gene expressions for 30,000 genes). Collectively, we can think of Observation s and Feature s as comprising the rows and columns of a two-dimensional matrix. We use Observation s and Feature s to hold metadata about data that we manipulating in MEV. We can attach attributes to these to allow users to set experimental groups, or other information usedful for visualization or filtering. These data structures have similar (if not exactly the same) behavior but we separate them for future compatability in case specialization of each class is needed. class api.data_structures.element. BaseElement ( id , attribute_dict={} ) A BaseElement is a base class from which we can derive both Observation and Features . For the purposes of clarity, we keep those entities separate. Yet, their behavior and structure are very much the same. This also allows us to add custom behavior to each at a later time if we require. An Element is structured as: { \"id\": <string identifier>, \"attributes\": { \"keyA\": <Attribute>, \"keyB\": <Attribute> } } class api.data_structures.observation. Observation ( id , attribute_dict={} ) An Observation is the generalization of a \"sample\" in the typical context of biological studies. One may think of samples and observations as interchangeable concepts. We call it an observation so that we are not limited by this convention, however. Observation instances act as metadata and can be used to filter and subset the data to which it is associated/attached. An Observation is structured as: { \"id\": <string identifier>, \"attributes\": { \"keyA\": <Attribute>, \"keyB\": <Attribute> } } class api.data_structures.feature. Feature ( id , attribute_dict={} ) A Feature can also be referred to as a covariate or variable. These are measurements one can make about an Observation . For example, in the genomics context, a sample can have 30,000+ genes which we call \"features\" here. In the statistical learning context, these are feature vectors. Feature instances act as metadata and can be used to filter and subset the data to which it is associated/attached. For example, we can imagine filtering by genes/features which have a particular value, such as those genes where the attribute \"oncogene\" is set to \"true\" A Feature is structured as: { \"id\": <string identifier>, \"attributes\": { \"keyA\": <Attribute>, \"keyB\": <Attribute> } }","title":"Observations and Features"},{"location":"elements/#elements-observations-and-features","text":"We adopt the convention from statistical learning of referring to Observation s and Feature s of data. Both of these data structures derive from BaseElement , which captures their common structure. Specialization specific to each can be overridden in the child classes. In an experimental context, Observation s are analogous to samples. That is, each Observation has one or more Feature s associated with it (e.g. gene expressions for 30,000 genes). Collectively, we can think of Observation s and Feature s as comprising the rows and columns of a two-dimensional matrix. We use Observation s and Feature s to hold metadata about data that we manipulating in MEV. We can attach attributes to these to allow users to set experimental groups, or other information usedful for visualization or filtering. These data structures have similar (if not exactly the same) behavior but we separate them for future compatability in case specialization of each class is needed. class api.data_structures.element. BaseElement ( id , attribute_dict={} ) A BaseElement is a base class from which we can derive both Observation and Features . For the purposes of clarity, we keep those entities separate. Yet, their behavior and structure are very much the same. This also allows us to add custom behavior to each at a later time if we require. An Element is structured as: { \"id\": <string identifier>, \"attributes\": { \"keyA\": <Attribute>, \"keyB\": <Attribute> } } class api.data_structures.observation. Observation ( id , attribute_dict={} ) An Observation is the generalization of a \"sample\" in the typical context of biological studies. One may think of samples and observations as interchangeable concepts. We call it an observation so that we are not limited by this convention, however. Observation instances act as metadata and can be used to filter and subset the data to which it is associated/attached. An Observation is structured as: { \"id\": <string identifier>, \"attributes\": { \"keyA\": <Attribute>, \"keyB\": <Attribute> } } class api.data_structures.feature. Feature ( id , attribute_dict={} ) A Feature can also be referred to as a covariate or variable. These are measurements one can make about an Observation . For example, in the genomics context, a sample can have 30,000+ genes which we call \"features\" here. In the statistical learning context, these are feature vectors. Feature instances act as metadata and can be used to filter and subset the data to which it is associated/attached. For example, we can imagine filtering by genes/features which have a particular value, such as those genes where the attribute \"oncogene\" is set to \"true\" A Feature is structured as: { \"id\": <string identifier>, \"attributes\": { \"keyA\": <Attribute>, \"keyB\": <Attribute> } }","title":"Elements, Observations, and Features"},{"location":"example_workflow/","text":"Example workflow To demonstrate how the various components of MEV come together, an graphical depiction of a typical workflow is shown below. The steps will be discussed in detail and connected to the various entities of the MEV architecture. Associated with each DataResource (AKA a file, depicted as rectangles above) is an ObservationSet , a FeatureSet , neither, or both. ObservationSet and FeatureSet s are essentially indexes on the columns/samples and the rows/genes as explained in Resource metadata . Step-by-step Denote the samples/columns of the original matrix (an instance of ObservationSet ) as all_observations . Similarly, denote all the rows/genes (an instance of FeatureSet ) as all_features . The original count matrix is run through the \"standard\"/automatic analyses. These are depicted using the gears and each are instances of Operation s. An Operation is essentially a function-- it has some input(s) and produces some output(s). Each of those Operation instances creates some output data/files. The content/format of those is not important here. Depending on the Operation , outputs could be flat files stored server-side (and served to the client) or simply data structures served to the client. One of those Operation s (PCA) allows you to create a \"selection\", which amounts to selecting a subset of all the samples. This is shown at point \"A\" in the figure. Through the UI, the user selects the desired samples (e.g. by clicking on points or dragging over areas of the PCA plot) and implicitly creates a client-side ObservationSet , which we will call pca_based_filter . This pca_based_filter is necessarily a subset of all_observations . Note that the user does not know about the concept of ObservationSet instances. Rather, they are simply selecting samples and choosing to group and label them according to some criteria (e.g. being clustered in a PCA plot). Also note that the dotted line in the figure is meant to suggest that pca_based_filter was \"inspired by\" by the PCA Operation , but did not actually derive from it. That is, while the visuals of the PCA plot were used to create the filter, the actual data of the PCA (the projected points) is not part of pca_based_filter (which is an ObservationSet ). Users can, however, name the ObservationSet so that they can be reminded of how these \"selections\" were made. At point \"B\", we apply that pca_based_filter to filter the columns of the original count matrix (recall that the columns of that original file is all_observations ). Although the icon is not a \"gear\", the green circle denoting the application of the filter is still an Operation in the MEV context. Also, note that we can apply the pca_based_filter filter to any existing file that has an ObservationSet . Obviously, it only provides a useful filter if there is a non-empty intersection of those sets; otherwise the filter produces an empty result. That is technically a valid Operation , however. At this point, the only existing DataResource /file is the original count matrix which has an ObservationSet we called all_observations . and we certainly have a non-empty intersection of the sets pca_based_filter and all_observations , so the filter is \"useful\". In accordance with our notion of an Observation , the filtering takes inputs (an ObservationSet and a DataResource ) and produces output(s); here the output is another DataResource which we call matrixA . In the backend, this creates both a physical file and a Resource in the database. Recall that a Resource is just a way for MEV to track files, but is agnostic of the \"behavior\" of the files. We next run a differential expression analysis (DESeq2) on matrixA . This produces a table of differential expression results. Note that when we choose to run DESeq2, we will be given the option of choosing from all available count matrices. In our case, that is either the original count matrix or the matrixA . We choose matrixA in the diagram. At point \"C\", we create a \"row filter\" by selecting the significantly differentially expressed genes from the DESeq2 results table. Recall that in our nomenclature we call this a FeatureSet . This FeatureSet (name it dge_fs ) can then be applied to any of the existing files where it makes sense. Again, by that I mean that it can be applied as a filter to any existing table that has a FeatureSet . Currently those are: original count matrix (where we called it all_features ) matrixA DESEq2 table Since we have not yet applied any row filters, all three of those DataResource s/files have FeatureSet s equivalent to all_features . The three files are shown flowing into node \"D\", but only one can be chosen (shown with solid line- matrixA ) At point \"D\", we apply dge_fs to matrixA in a filtering Operation . This produces a new file which we call matrixB . If you're keeping score, matrixB is basically the original table with both a row and column filter applied. We then run analyses on matrixB , such as a new PCA and a GSEA analysis. Additional notes This way of operation ends up producing multiple files that copy portions of the original matrix. We could try and be slick and store those filter operations, but it's easier to just write new files. Allowing multiple DataResource s/files within a Workspace allows us to use multiple sources of data within an analysis. In the older iterations MEV, all the analyses have to \"flow\" from a single original file. This is more or less what we did in the figure above, but we are no longer constrained to operate in that way. One could imagine adding a VCF file to the Workspace which might allow one to perform an eQTL analysis, for example.","title":"Workflow and analysis concepts"},{"location":"example_workflow/#example-workflow","text":"To demonstrate how the various components of MEV come together, an graphical depiction of a typical workflow is shown below. The steps will be discussed in detail and connected to the various entities of the MEV architecture. Associated with each DataResource (AKA a file, depicted as rectangles above) is an ObservationSet , a FeatureSet , neither, or both. ObservationSet and FeatureSet s are essentially indexes on the columns/samples and the rows/genes as explained in Resource metadata . Step-by-step Denote the samples/columns of the original matrix (an instance of ObservationSet ) as all_observations . Similarly, denote all the rows/genes (an instance of FeatureSet ) as all_features . The original count matrix is run through the \"standard\"/automatic analyses. These are depicted using the gears and each are instances of Operation s. An Operation is essentially a function-- it has some input(s) and produces some output(s). Each of those Operation instances creates some output data/files. The content/format of those is not important here. Depending on the Operation , outputs could be flat files stored server-side (and served to the client) or simply data structures served to the client. One of those Operation s (PCA) allows you to create a \"selection\", which amounts to selecting a subset of all the samples. This is shown at point \"A\" in the figure. Through the UI, the user selects the desired samples (e.g. by clicking on points or dragging over areas of the PCA plot) and implicitly creates a client-side ObservationSet , which we will call pca_based_filter . This pca_based_filter is necessarily a subset of all_observations . Note that the user does not know about the concept of ObservationSet instances. Rather, they are simply selecting samples and choosing to group and label them according to some criteria (e.g. being clustered in a PCA plot). Also note that the dotted line in the figure is meant to suggest that pca_based_filter was \"inspired by\" by the PCA Operation , but did not actually derive from it. That is, while the visuals of the PCA plot were used to create the filter, the actual data of the PCA (the projected points) is not part of pca_based_filter (which is an ObservationSet ). Users can, however, name the ObservationSet so that they can be reminded of how these \"selections\" were made. At point \"B\", we apply that pca_based_filter to filter the columns of the original count matrix (recall that the columns of that original file is all_observations ). Although the icon is not a \"gear\", the green circle denoting the application of the filter is still an Operation in the MEV context. Also, note that we can apply the pca_based_filter filter to any existing file that has an ObservationSet . Obviously, it only provides a useful filter if there is a non-empty intersection of those sets; otherwise the filter produces an empty result. That is technically a valid Operation , however. At this point, the only existing DataResource /file is the original count matrix which has an ObservationSet we called all_observations . and we certainly have a non-empty intersection of the sets pca_based_filter and all_observations , so the filter is \"useful\". In accordance with our notion of an Observation , the filtering takes inputs (an ObservationSet and a DataResource ) and produces output(s); here the output is another DataResource which we call matrixA . In the backend, this creates both a physical file and a Resource in the database. Recall that a Resource is just a way for MEV to track files, but is agnostic of the \"behavior\" of the files. We next run a differential expression analysis (DESeq2) on matrixA . This produces a table of differential expression results. Note that when we choose to run DESeq2, we will be given the option of choosing from all available count matrices. In our case, that is either the original count matrix or the matrixA . We choose matrixA in the diagram. At point \"C\", we create a \"row filter\" by selecting the significantly differentially expressed genes from the DESeq2 results table. Recall that in our nomenclature we call this a FeatureSet . This FeatureSet (name it dge_fs ) can then be applied to any of the existing files where it makes sense. Again, by that I mean that it can be applied as a filter to any existing table that has a FeatureSet . Currently those are: original count matrix (where we called it all_features ) matrixA DESEq2 table Since we have not yet applied any row filters, all three of those DataResource s/files have FeatureSet s equivalent to all_features . The three files are shown flowing into node \"D\", but only one can be chosen (shown with solid line- matrixA ) At point \"D\", we apply dge_fs to matrixA in a filtering Operation . This produces a new file which we call matrixB . If you're keeping score, matrixB is basically the original table with both a row and column filter applied. We then run analyses on matrixB , such as a new PCA and a GSEA analysis. Additional notes This way of operation ends up producing multiple files that copy portions of the original matrix. We could try and be slick and store those filter operations, but it's easier to just write new files. Allowing multiple DataResource s/files within a Workspace allows us to use multiple sources of data within an analysis. In the older iterations MEV, all the analyses have to \"flow\" from a single original file. This is more or less what we did in the figure above, but we are no longer constrained to operate in that way. One could imagine adding a VCF file to the Workspace which might allow one to perform an eQTL analysis, for example.","title":"Example workflow"},{"location":"install/","text":"Installation instructions Dockerhub image TODO Local build The application is packaged as a Docker for easier management of dependencies. To build the Docker image yourself, you will need to have Docker installed locally. To build the application, clone the repository locally: git clone https://github.com/hsph-qbrc/mev-backend.git Navigate into that cloned directory and build the Docker image with: docker build -t <image name> . To run the container, you will need to supply some environment variables for things like passwords and other sensitive information. In the repository is a file named env_vars.template.txt . Fill that in with the various usernames and passwords as you like. In particular, if you are running a local development version, you will need to specify: ENVIRONMENT=dev Additionally, specify the same value (an email address) for both DJANGO_SUPERUSER_EMAIL and DJANGO_SUPERUSER_USERNAME . This is because MEV uses emails as the unique usernames. If you would like some \"dummy\" data to be entered into the database (e.g. for developing a front-end), you must also specify POPULUATE_DB=yes (case-sensitive!). Any other values for this variable will skip the population of dummy data. To run the container: docker run -it --env-file <ENV VARS> -p8000:8000 <DOCKER IMAGE NAME> --bind 0.0.0.0:8000 Note the following: We bind port 8000 inside the container to 8000 outside the container. Due to the use of ENTRYPOINT in the Dockerfile, the commands following the image name (e.g. --bind 0.0.0.0:8000 above) will be appended to the command: gunicorn mev.wsgi:application Therefore, for the example given above, the full command on startup would then be, gunicorn mev.wsgi:application --bind 0.0.0.0:8000 which starts gunicorn and listens for connections on 0.0.0.0:8000 . This allows one to access the API from outside the container. Note that additional gunicorn configuration parameters can be specified (see https://docs.gunicorn.org/en/latest/configure.html). At this point you may go to your browser and navigate to http://127.0.0.1:8000/api/. You will need to login with the username (email) and password you gave in your file of environment variables.","title":"Install"},{"location":"install/#installation-instructions","text":"","title":"Installation instructions"},{"location":"install/#dockerhub-image","text":"TODO","title":"Dockerhub image"},{"location":"install/#local-build","text":"The application is packaged as a Docker for easier management of dependencies. To build the Docker image yourself, you will need to have Docker installed locally. To build the application, clone the repository locally: git clone https://github.com/hsph-qbrc/mev-backend.git Navigate into that cloned directory and build the Docker image with: docker build -t <image name> . To run the container, you will need to supply some environment variables for things like passwords and other sensitive information. In the repository is a file named env_vars.template.txt . Fill that in with the various usernames and passwords as you like. In particular, if you are running a local development version, you will need to specify: ENVIRONMENT=dev Additionally, specify the same value (an email address) for both DJANGO_SUPERUSER_EMAIL and DJANGO_SUPERUSER_USERNAME . This is because MEV uses emails as the unique usernames. If you would like some \"dummy\" data to be entered into the database (e.g. for developing a front-end), you must also specify POPULUATE_DB=yes (case-sensitive!). Any other values for this variable will skip the population of dummy data. To run the container: docker run -it --env-file <ENV VARS> -p8000:8000 <DOCKER IMAGE NAME> --bind 0.0.0.0:8000 Note the following: We bind port 8000 inside the container to 8000 outside the container. Due to the use of ENTRYPOINT in the Dockerfile, the commands following the image name (e.g. --bind 0.0.0.0:8000 above) will be appended to the command: gunicorn mev.wsgi:application Therefore, for the example given above, the full command on startup would then be, gunicorn mev.wsgi:application --bind 0.0.0.0:8000 which starts gunicorn and listens for connections on 0.0.0.0:8000 . This allows one to access the API from outside the container. Note that additional gunicorn configuration parameters can be specified (see https://docs.gunicorn.org/en/latest/configure.html). At this point you may go to your browser and navigate to http://127.0.0.1:8000/api/. You will need to login with the username (email) and password you gave in your file of environment variables.","title":"Local build"},{"location":"operations/","text":"Operations and ExecutedOperations An Operation is any manipulation of some data that produces some output; it defines the type of analysis that is run, its inputs and outputs, and other relevant information. An Operation can be as simple as selecting a subset of the columns/rows of a matrix or running a large-scale processing job that spans many machines and significant time. An ExecutedOperation represents an actual execution of an Operation . While the Operation identifies the process used, the ExecutedOperation contains information about the actual execution, such as the job status, the exact inputs and outputs, the runtime, and other relevant information. Clearly, the ExecutedOperation maintains a foreign-key relation to the Operation . The various ExecutedOperation s performed in MEV will all create some output so that there will be no ambiguity regarding how data was manipulated through the course of an analysis workflow. Essentially, we do not perform in-place operations on data. For example, if a user chooses a subset of samples/columns in their expression matrix, we create a new DataResource (and corresponding Resource database record). While this has the potential to create multiple files with similar data, this makes auditing a workflow history much simpler. Typically, the size of files where users are interacting with the data are relatively small (on the order of MB) so excessive storage is not a major concern. Note that client-side manipulations of the data, such as filtering out samples are distinct from this concept of an Operation / ExecutedOperation . That is, users can select various filters on their data to change the visualizations without executing any Operation s. However , once they choose to use the subset data for use in an analysis, they will be required to implicitly execute an Operation on the backend. As a concrete example, consider analyzing a large cohort of expression data from TCGA. A user initially imports the expression matrix and perhaps uses PCA or some other clustering method in an exploratory analysis. Each of those initial analyses were ExecutedOperation s. Based on those initial visualizations, the user may select a subset of those samples to investigate for potential subtypes; note that those client-side sample selections have not triggered any actual analyses. However, once they choose to run those samples through a differential expression analysis, we require that they perform filter/subset Operation . As new DataResource s are created, the metadata tracks which ExecutedOperation created them, addressed by the UUID assigned to each ExecutedOperation . By tracing the foreign-key relation, we can determine the exact Operation that was run so that the steps of the analysis are transparent and reproducible. Operation s can be lightweight jobs such as a basic filter or a simple R script, or involve complex, multi-step pipelines orchestrated using the CNAP-style workflows involving WDL and Cromwell. Depending on the computational resources needed, the Operation can be run locally or remotely. As jobs complete, their outputs will populate in the user's workspace and further analysis can be performed. All jobs, whether local or remote, will be placed in a queue and executed ascynchronously. Progress/status of remote jobs can be monitored by querying the Cromwell server. Also note that ALL Operation s (even basic table filtering) are executed in Docker containers so that the software and environments can be carefully tracked and controlled. This ensures a consistent \"plugin\" style architecture so that new Operation s can be integrated consistently. Operation s should maintain the following data: unique identifier (UUID) a simplier \"version\" identifier (e.g. \"v1\"). This allows us to change analyses over time but let users recreate earlier analyses so that their processing is consistent/reproducible. name description of the analysis Inputs to the analysis. These are the acceptable types and potentially some parameters. For instance, a DESeq2 analysis should take an IntegerMatrix , two ObservationSet instances (defining the contrast groups), and a string \"name\" for the contrast. Outputs of the analysis. This would be the \"types\" of outputs. Again, using the DESEq2 example, the outputs could be a TableResource giving the table of differentially expressed genes (e.g. fold-change, p-values) and a normalized expression matrix of type Matrix . github repo/Docker info (commit hashes) so that the backend analysis code may be traced whether the Operation is a \"local\" one or requires use of the Cromwell engine. ExecutedOperation s should maintain the following data: The Workspace (which also gives access to the user/owner) a foreign-key to the Operation \"type\" unique identifier (UUID) for the execution a job identifier if the analysis is run remotely (on Cromwell). We need the Cromwell job UUID to track the progress as we query the Cromwell server. The inputs to the analysis (e.g. the UUIDs of the Resource s that were input) The outputs (more UUIDs of the Resource s created) once complete Job execution status (e.g. \"running\", \"complete\", \"failed\", etc.) Start time Completion time Any errors or warnings","title":"Operations"},{"location":"operations/#operations-and-executedoperations","text":"An Operation is any manipulation of some data that produces some output; it defines the type of analysis that is run, its inputs and outputs, and other relevant information. An Operation can be as simple as selecting a subset of the columns/rows of a matrix or running a large-scale processing job that spans many machines and significant time. An ExecutedOperation represents an actual execution of an Operation . While the Operation identifies the process used, the ExecutedOperation contains information about the actual execution, such as the job status, the exact inputs and outputs, the runtime, and other relevant information. Clearly, the ExecutedOperation maintains a foreign-key relation to the Operation . The various ExecutedOperation s performed in MEV will all create some output so that there will be no ambiguity regarding how data was manipulated through the course of an analysis workflow. Essentially, we do not perform in-place operations on data. For example, if a user chooses a subset of samples/columns in their expression matrix, we create a new DataResource (and corresponding Resource database record). While this has the potential to create multiple files with similar data, this makes auditing a workflow history much simpler. Typically, the size of files where users are interacting with the data are relatively small (on the order of MB) so excessive storage is not a major concern. Note that client-side manipulations of the data, such as filtering out samples are distinct from this concept of an Operation / ExecutedOperation . That is, users can select various filters on their data to change the visualizations without executing any Operation s. However , once they choose to use the subset data for use in an analysis, they will be required to implicitly execute an Operation on the backend. As a concrete example, consider analyzing a large cohort of expression data from TCGA. A user initially imports the expression matrix and perhaps uses PCA or some other clustering method in an exploratory analysis. Each of those initial analyses were ExecutedOperation s. Based on those initial visualizations, the user may select a subset of those samples to investigate for potential subtypes; note that those client-side sample selections have not triggered any actual analyses. However, once they choose to run those samples through a differential expression analysis, we require that they perform filter/subset Operation . As new DataResource s are created, the metadata tracks which ExecutedOperation created them, addressed by the UUID assigned to each ExecutedOperation . By tracing the foreign-key relation, we can determine the exact Operation that was run so that the steps of the analysis are transparent and reproducible. Operation s can be lightweight jobs such as a basic filter or a simple R script, or involve complex, multi-step pipelines orchestrated using the CNAP-style workflows involving WDL and Cromwell. Depending on the computational resources needed, the Operation can be run locally or remotely. As jobs complete, their outputs will populate in the user's workspace and further analysis can be performed. All jobs, whether local or remote, will be placed in a queue and executed ascynchronously. Progress/status of remote jobs can be monitored by querying the Cromwell server. Also note that ALL Operation s (even basic table filtering) are executed in Docker containers so that the software and environments can be carefully tracked and controlled. This ensures a consistent \"plugin\" style architecture so that new Operation s can be integrated consistently. Operation s should maintain the following data: unique identifier (UUID) a simplier \"version\" identifier (e.g. \"v1\"). This allows us to change analyses over time but let users recreate earlier analyses so that their processing is consistent/reproducible. name description of the analysis Inputs to the analysis. These are the acceptable types and potentially some parameters. For instance, a DESeq2 analysis should take an IntegerMatrix , two ObservationSet instances (defining the contrast groups), and a string \"name\" for the contrast. Outputs of the analysis. This would be the \"types\" of outputs. Again, using the DESEq2 example, the outputs could be a TableResource giving the table of differentially expressed genes (e.g. fold-change, p-values) and a normalized expression matrix of type Matrix . github repo/Docker info (commit hashes) so that the backend analysis code may be traced whether the Operation is a \"local\" one or requires use of the Cromwell engine. ExecutedOperation s should maintain the following data: The Workspace (which also gives access to the user/owner) a foreign-key to the Operation \"type\" unique identifier (UUID) for the execution a job identifier if the analysis is run remotely (on Cromwell). We need the Cromwell job UUID to track the progress as we query the Cromwell server. The inputs to the analysis (e.g. the UUIDs of the Resource s that were input) The outputs (more UUIDs of the Resource s created) once complete Job execution status (e.g. \"running\", \"complete\", \"failed\", etc.) Start time Completion time Any errors or warnings","title":"Operations and ExecutedOperations"},{"location":"resource_metadata/","text":"Resource metadata Metadata can be associated with type of DataResource . Note that a DataResource is related, but distinct from a Resource . The latter is for tracking the various file-based resources in the database; it knows about the file location, size, and the type of the resource (as a string field). The former is a base class from which the many specialized \"types\" of resources derive. For instance, an IntegerMatrix derives from a DataResource . Instead of being a database record, a DataResource captures the expected format and behavior of the resource. For instance, the children classes of DataResource contain validators and parsers. Thus, associated with each DataResource is some metadata. The specification may expand to incorporate additional fields, but at minimum, it should contain: An ObservationSet . For a FastQ file representing a single sample (most common case), the ObservationSet would have a single item (of type Observation ) containing information about that particular sample. For a count matrix of size (p, N), the ObservationSet would have N items (again, of type Observation ) giving information about the samples in the columns. A FeatureSet . This is a collection of covariates corresponding to a single Observation . A Feature is something that is measured (e.g. read counts for a gene). For a count matrix of size (p, N), the FeatureSet would have p items (of type Feature ) and correspond to the p genes measured for a single sample. For a sequence-based file like a FastQ, this would simply be null; perhaps there are alternative intepretations of this concept, but the point is that the field can be null. A table of differentially expressed genes would have a FeatureSet , but not an ObservationSet ; in this case the Feature s are the genes and we are given information like log-fold change and p-value. A parent operation. As an analysis workflow can be represented as a directed, acyclic graph (DAG), we would like to track the flow of data and operations on the data. Tracking the \"parent\" of a DataResource allows us to determine which operation generated the data and hence reconstruct the full DAG. The original input files would have a null parent. We maintain a \"master copy\" of the metadata on the server side as a flat file for reference. We do not want to have to repeatedly open/parse a large text file to determine the rows/features and columns/observations. We imagine that for performance reasons, client-applications may choose to cache this metadata so that desired sets of rows or columns can be selected on the client side without involving a request to the server. Requests to subset/filter a DataResource would provide ObservationSet s or FeatureSet s which are compared against the respective ObservationSet s or FeatureSet s of the DataResource .","title":"Resource metadata"},{"location":"resource_metadata/#resource-metadata","text":"Metadata can be associated with type of DataResource . Note that a DataResource is related, but distinct from a Resource . The latter is for tracking the various file-based resources in the database; it knows about the file location, size, and the type of the resource (as a string field). The former is a base class from which the many specialized \"types\" of resources derive. For instance, an IntegerMatrix derives from a DataResource . Instead of being a database record, a DataResource captures the expected format and behavior of the resource. For instance, the children classes of DataResource contain validators and parsers. Thus, associated with each DataResource is some metadata. The specification may expand to incorporate additional fields, but at minimum, it should contain: An ObservationSet . For a FastQ file representing a single sample (most common case), the ObservationSet would have a single item (of type Observation ) containing information about that particular sample. For a count matrix of size (p, N), the ObservationSet would have N items (again, of type Observation ) giving information about the samples in the columns. A FeatureSet . This is a collection of covariates corresponding to a single Observation . A Feature is something that is measured (e.g. read counts for a gene). For a count matrix of size (p, N), the FeatureSet would have p items (of type Feature ) and correspond to the p genes measured for a single sample. For a sequence-based file like a FastQ, this would simply be null; perhaps there are alternative intepretations of this concept, but the point is that the field can be null. A table of differentially expressed genes would have a FeatureSet , but not an ObservationSet ; in this case the Feature s are the genes and we are given information like log-fold change and p-value. A parent operation. As an analysis workflow can be represented as a directed, acyclic graph (DAG), we would like to track the flow of data and operations on the data. Tracking the \"parent\" of a DataResource allows us to determine which operation generated the data and hence reconstruct the full DAG. The original input files would have a null parent. We maintain a \"master copy\" of the metadata on the server side as a flat file for reference. We do not want to have to repeatedly open/parse a large text file to determine the rows/features and columns/observations. We imagine that for performance reasons, client-applications may choose to cache this metadata so that desired sets of rows or columns can be selected on the client side without involving a request to the server. Requests to subset/filter a DataResource would provide ObservationSet s or FeatureSet s which are compared against the respective ObservationSet s or FeatureSet s of the DataResource .","title":"Resource metadata"},{"location":"resource_types/","text":"Resource types A Resource represents some generic notion of data and its resource_type field/member is a string identifier that identifies the specific format of the data. The string identifiers map to concrete classes that implement validation methods for the Resource . When a new Resource is added (via upload or directly by an admin via the API), the validation method is called. Similarly, if a user tries to change the resource_type , it will trigger the validation process. Current resource_types fall into two broad categories: Table-based formats Sequence-based formats Table-based formats are any array-like format, such as a typical CSV file. This covers a wide variety of standard formats encountered in computational biology, including GTF annotation files and BED files. The primitive data types contained in each column are determined using Python's Pandas library, which refers to these as \"dtypes\"; for example, a column identified as int64 certainly qualifies as an integer type. If the column contains any non-integers (but all numbers), Pandas automatically converts it to a float type (e.g. float64 ) which allows us to easily validate the content of each column. Sequence-based formats are formats like FastQ, Fasta, or SAM/BAM. Table-based resource types class resource_types.table_types. TableResource ( ) The TableResource is the most generic form of a delimited file. Any type of data that can be represented as rows and columns. This or any of the more specific subclasses can be contained in files saved in CSV, TSV, or Excel (xls/xlsx) format. If in Excel format, the data of interest must reside in the first sheet of the workbook. Special tab-delimited files like BED or VCF files are recognized by their canonical extension (e.g. \".bed\" or \".vcf\"). Note that unless you create a \"specialized\" implementation (e.g. like for a BED file), then we assume you have features as rows and observables as columns. class resource_types.table_types. Matrix ( ) A Matrix is a delimited table-based file that has only numeric types. These types can be mixed, like floats and integers class resource_types.table_types. IntegerMatrix ( ) An IntegerMatrix further specializes the Matrix to admit only integers. class resource_types.table_types. AnnotationTable ( ) An AnnotationTable is a special type of table that will be responsible for annotating Observations/samples (e.g. adding sample names and associated attributes like experimental group or other covariates) The first column will give the sample names and the remaining columns will each individually represent different covariates associated with that sample. For example, if we received the following table: sample genotype treatment A WT Y B WT N Then this table can be used to create Attribute s which can be added to the Observation s. After the annotations are uploaded, the users must tell MEV how to interpret the columns (e.g. as a string? as a bounded float?), but once that type is specified, we can validate the annotations against that choice and subsequently add the Attribute s to the Observation s. class resource_types.table_types. BEDFile ( ) A file format that corresponds to the BED format. This is the minimal BED format, which has: chromosome start position end position Additional columns are ignored. By default, BED files do NOT contain headers and we enforce that here. Sequence-based formats class resource_types.sequence_types. SequenceResource ( ) This class is used to represent sequence-based files such as Fasta, Fastq, SAM/BAM We cannot (reasonably) locally validate the contents of these files quickly or exhaustively, so minimal validation is performed remotely class resource_types.sequence_types. FastAResource ( ) This type is for validating Fasta files, compressed or not. Fasta files are recognized using the following formats: fasta fasta.gz fa fa.gz class resource_types.sequence_types. FastQResource ( ) This resource type is for Fastq files, compressed or not. Fastq files are recognized using the following formats: fastq fastq.gz fq fq.gz class resource_types.sequence_types. AlignedSequenceResource ( ) This resource type is for SAM/BAM files. We accept both SAM and BAM files named using their canonical extensions: \".bam\" for BAM files \".sam\" for SAM files","title":"Resource types"},{"location":"resource_types/#resource-types","text":"A Resource represents some generic notion of data and its resource_type field/member is a string identifier that identifies the specific format of the data. The string identifiers map to concrete classes that implement validation methods for the Resource . When a new Resource is added (via upload or directly by an admin via the API), the validation method is called. Similarly, if a user tries to change the resource_type , it will trigger the validation process. Current resource_types fall into two broad categories: Table-based formats Sequence-based formats Table-based formats are any array-like format, such as a typical CSV file. This covers a wide variety of standard formats encountered in computational biology, including GTF annotation files and BED files. The primitive data types contained in each column are determined using Python's Pandas library, which refers to these as \"dtypes\"; for example, a column identified as int64 certainly qualifies as an integer type. If the column contains any non-integers (but all numbers), Pandas automatically converts it to a float type (e.g. float64 ) which allows us to easily validate the content of each column. Sequence-based formats are formats like FastQ, Fasta, or SAM/BAM.","title":"Resource types"},{"location":"resource_types/#table-based-resource-types","text":"class resource_types.table_types. TableResource ( ) The TableResource is the most generic form of a delimited file. Any type of data that can be represented as rows and columns. This or any of the more specific subclasses can be contained in files saved in CSV, TSV, or Excel (xls/xlsx) format. If in Excel format, the data of interest must reside in the first sheet of the workbook. Special tab-delimited files like BED or VCF files are recognized by their canonical extension (e.g. \".bed\" or \".vcf\"). Note that unless you create a \"specialized\" implementation (e.g. like for a BED file), then we assume you have features as rows and observables as columns. class resource_types.table_types. Matrix ( ) A Matrix is a delimited table-based file that has only numeric types. These types can be mixed, like floats and integers class resource_types.table_types. IntegerMatrix ( ) An IntegerMatrix further specializes the Matrix to admit only integers. class resource_types.table_types. AnnotationTable ( ) An AnnotationTable is a special type of table that will be responsible for annotating Observations/samples (e.g. adding sample names and associated attributes like experimental group or other covariates) The first column will give the sample names and the remaining columns will each individually represent different covariates associated with that sample. For example, if we received the following table: sample genotype treatment A WT Y B WT N Then this table can be used to create Attribute s which can be added to the Observation s. After the annotations are uploaded, the users must tell MEV how to interpret the columns (e.g. as a string? as a bounded float?), but once that type is specified, we can validate the annotations against that choice and subsequently add the Attribute s to the Observation s. class resource_types.table_types. BEDFile ( ) A file format that corresponds to the BED format. This is the minimal BED format, which has: chromosome start position end position Additional columns are ignored. By default, BED files do NOT contain headers and we enforce that here.","title":"Table-based resource types"},{"location":"resource_types/#sequence-based-formats","text":"class resource_types.sequence_types. SequenceResource ( ) This class is used to represent sequence-based files such as Fasta, Fastq, SAM/BAM We cannot (reasonably) locally validate the contents of these files quickly or exhaustively, so minimal validation is performed remotely class resource_types.sequence_types. FastAResource ( ) This type is for validating Fasta files, compressed or not. Fasta files are recognized using the following formats: fasta fasta.gz fa fa.gz class resource_types.sequence_types. FastQResource ( ) This resource type is for Fastq files, compressed or not. Fastq files are recognized using the following formats: fastq fastq.gz fq fq.gz class resource_types.sequence_types. AlignedSequenceResource ( ) This resource type is for SAM/BAM files. We accept both SAM and BAM files named using their canonical extensions: \".bam\" for BAM files \".sam\" for SAM files","title":"Sequence-based formats"},{"location":"resources/","text":"Resources Much of the information regarding Resource instances is provided in the auto-generated docstring below, but here we highlight some key elements of the Resource model. Namely, the kinds of operations users and admins can take to create, delete, or otherwise manipulated Resource s via the API. Resource creation Regular MEV users can only create Resource instances by uploading files, either via a direct method (upload from local machine) or by using one our cloud-based uploaders. They can't do this via the API. Admins can \"override\" and create Resource instances manually via the API. Regardless of who created the Resource , the validation process is started asynchronously. We cannot assume that the files are properly validated, even if the request was initiated by an admin. Upon creation of the Resource , it is immediately set to \"inactive\" ( is_active = False ) while we validate the particular type. Resource instances have a single owner, which is the owner who uploaded the file, or directly specified by the admin in the API request. Resource \"type\" A Resource is required to have a \"type\" (e.g. an integer matrix) which we call a resource_type . These types are restricted to a set of common file formats. Upon creation, resource_type is set to None which indicates that the Resource has not been validated. The type of the Resource can be specified when the file is uploaded or at any other time (i.e. users can change the type if they desire). Each request to change type initiates an asynchronous validation process. If the validation of the resource_type fails, we revert back to the previous successfully validated type. If the type was previously None (as with a new upload), we simply revert back to None and inform the user the validation failed. Resources and Workspaces Resource instances are initially \"unattached\" meaning they are associated with their owner, but have not been associated with any user workspaces. Admins can, however, specify a Workspace in their request to create the Resource directly via the API. When a user chooses to \"add\" a Resource to a Workspace , a new database record is created which is a copy of the original, unattached Resource with the same attributes except the unique Resource UUID. Thus, we have two database records referencing the same file. We could accomplish something similar with a many-to-one mapping of Workspace to Resource s, but this was a choice we made which could allow for resource-copying if we ever allow file-editing in the future. In that case, attaching a Resource to a Workspace could create a copy of the file such that the original Resource remains unaltered. The user can, of course, change any of the mutable members of this new Workspace -associated Resource . The changes will be independent of the original \"unattached\" Resource . Users can remove a Resource from a Workspace if it has NOT been used for any portions of the analysis. We want to retain the completeness of the analysis, so deleting files that are part of the analysis \"tree\" would create gaps. Deletion of Resources Since multiple database records can reference the same underlying file, we have a bit of custom logic for determining when we delete only the database record versus deleting the actual underlying file. Essentially, if a deletion is requested and no other Resource database records reference the same file, then we delete both the database record AND the file. In the case where there is another database record referencing that file, we only remove the database record, leaving the file. Notes related to backend implementation In general, the is_active = False flag disallows any updating of the Resource attributes via the API. All post/patch/put requests will return a 400 status. This prevents multiple requests from interfering with an ongoing background process. Users cannot change the path member. The actual storage of the files should not matter to the users so they are unable to change the path member. class api.models.resource. Resource ( *args , **kwargs ) A Resource is an abstraction of data. It represents some piece of data we are analyzing or manipulating in the course of an analysis workflow. Resource s are most often represented by flat files, but their physical storage is not important. They could be stored locally or in cloud storage accessible to MEV. Various \"types\" of Resource s implement specific constraints on the data that are important for tracking inputs and outputs of analyses. For example, if an analysis module needs to operate on a matrix of integers, we can enforce that the only Resource s available as inputs are those identified (and verified) as IntegerMatrix \"types\". Note that we store all types of Resource s in the database as a single table and maintain the notion of \"type\" by a string-field identifier. Creating specific database tables for each type of Resource would be unnecessary. By connecting the string stored in the database with a concrete implementation class we can check the type of the Resource . Resource s are not active ( is_active flag in the database) until their \"type\" has been verified. API users will submit the intended type with the request and the backend will check that. Violations are reported and the Resource remains inactive ( is_active=False ). Some additional notes: Resource s are owned by users and can be added to a Workspace . However, that is not required-- Resource s can be \"unattached\". Regular users (non-admins) can't create new Resource directly via the API. The only way they can create a Resource is indirectly by adding a new upload. When a Resource is added to a Workspace , a new copy of the database record is made. This maintains the state of the original Resource . Resource s can be made \"public\" so that others can view and import them. Once another user chooses to import the file, a copy is made and that new user has their own copy. If a Resource is later made \"private\" then any files that have been \"used\" by others cannot be recalled. Resource s can be removed from a Workspace , but only if they have not been used for any analyses/operations. Resource s cannot be transferred from one Workspace to another, but they can be copied. A change in the type of the Resource can be requested. Until the validation of that change is complete, the Resource is made private and inactive. Admins can make essentially any change to Resources , including creation. However, they must be careful to maintain the integrity of the database and the files they point to. In a request to create a Resource via the API, the resource_type field can be blank/null. The type can be inferred from the path of the resource. We can do this because only admins are allowed to create via the API and they should only generate such requests if the resource type can be inferred (i.e. admins know not to give bad requests to the API...)","title":"General info"},{"location":"resources/#resources","text":"Much of the information regarding Resource instances is provided in the auto-generated docstring below, but here we highlight some key elements of the Resource model. Namely, the kinds of operations users and admins can take to create, delete, or otherwise manipulated Resource s via the API. Resource creation Regular MEV users can only create Resource instances by uploading files, either via a direct method (upload from local machine) or by using one our cloud-based uploaders. They can't do this via the API. Admins can \"override\" and create Resource instances manually via the API. Regardless of who created the Resource , the validation process is started asynchronously. We cannot assume that the files are properly validated, even if the request was initiated by an admin. Upon creation of the Resource , it is immediately set to \"inactive\" ( is_active = False ) while we validate the particular type. Resource instances have a single owner, which is the owner who uploaded the file, or directly specified by the admin in the API request. Resource \"type\" A Resource is required to have a \"type\" (e.g. an integer matrix) which we call a resource_type . These types are restricted to a set of common file formats. Upon creation, resource_type is set to None which indicates that the Resource has not been validated. The type of the Resource can be specified when the file is uploaded or at any other time (i.e. users can change the type if they desire). Each request to change type initiates an asynchronous validation process. If the validation of the resource_type fails, we revert back to the previous successfully validated type. If the type was previously None (as with a new upload), we simply revert back to None and inform the user the validation failed. Resources and Workspaces Resource instances are initially \"unattached\" meaning they are associated with their owner, but have not been associated with any user workspaces. Admins can, however, specify a Workspace in their request to create the Resource directly via the API. When a user chooses to \"add\" a Resource to a Workspace , a new database record is created which is a copy of the original, unattached Resource with the same attributes except the unique Resource UUID. Thus, we have two database records referencing the same file. We could accomplish something similar with a many-to-one mapping of Workspace to Resource s, but this was a choice we made which could allow for resource-copying if we ever allow file-editing in the future. In that case, attaching a Resource to a Workspace could create a copy of the file such that the original Resource remains unaltered. The user can, of course, change any of the mutable members of this new Workspace -associated Resource . The changes will be independent of the original \"unattached\" Resource . Users can remove a Resource from a Workspace if it has NOT been used for any portions of the analysis. We want to retain the completeness of the analysis, so deleting files that are part of the analysis \"tree\" would create gaps. Deletion of Resources Since multiple database records can reference the same underlying file, we have a bit of custom logic for determining when we delete only the database record versus deleting the actual underlying file. Essentially, if a deletion is requested and no other Resource database records reference the same file, then we delete both the database record AND the file. In the case where there is another database record referencing that file, we only remove the database record, leaving the file. Notes related to backend implementation In general, the is_active = False flag disallows any updating of the Resource attributes via the API. All post/patch/put requests will return a 400 status. This prevents multiple requests from interfering with an ongoing background process. Users cannot change the path member. The actual storage of the files should not matter to the users so they are unable to change the path member. class api.models.resource. Resource ( *args , **kwargs ) A Resource is an abstraction of data. It represents some piece of data we are analyzing or manipulating in the course of an analysis workflow. Resource s are most often represented by flat files, but their physical storage is not important. They could be stored locally or in cloud storage accessible to MEV. Various \"types\" of Resource s implement specific constraints on the data that are important for tracking inputs and outputs of analyses. For example, if an analysis module needs to operate on a matrix of integers, we can enforce that the only Resource s available as inputs are those identified (and verified) as IntegerMatrix \"types\". Note that we store all types of Resource s in the database as a single table and maintain the notion of \"type\" by a string-field identifier. Creating specific database tables for each type of Resource would be unnecessary. By connecting the string stored in the database with a concrete implementation class we can check the type of the Resource . Resource s are not active ( is_active flag in the database) until their \"type\" has been verified. API users will submit the intended type with the request and the backend will check that. Violations are reported and the Resource remains inactive ( is_active=False ). Some additional notes: Resource s are owned by users and can be added to a Workspace . However, that is not required-- Resource s can be \"unattached\". Regular users (non-admins) can't create new Resource directly via the API. The only way they can create a Resource is indirectly by adding a new upload. When a Resource is added to a Workspace , a new copy of the database record is made. This maintains the state of the original Resource . Resource s can be made \"public\" so that others can view and import them. Once another user chooses to import the file, a copy is made and that new user has their own copy. If a Resource is later made \"private\" then any files that have been \"used\" by others cannot be recalled. Resource s can be removed from a Workspace , but only if they have not been used for any analyses/operations. Resource s cannot be transferred from one Workspace to another, but they can be copied. A change in the type of the Resource can be requested. Until the validation of that change is complete, the Resource is made private and inactive. Admins can make essentially any change to Resources , including creation. However, they must be careful to maintain the integrity of the database and the files they point to. In a request to create a Resource via the API, the resource_type field can be blank/null. The type can be inferred from the path of the resource. We can do this because only admins are allowed to create via the API and they should only generate such requests if the resource type can be inferred (i.e. admins know not to give bad requests to the API...)","title":"Resources"},{"location":"workspaces/","text":"Workspaces class api.models.workspace. Workspace ( *args , **kwargs ) A Workspace is a way to logically group the files and and analyses that are part of a user's work. Users can have multiple Workspace s to separate distinct analyses. Data, files, and analyses are grouped under a Workspace such that all information related to the analyses, including analysis history, is captured the Workspace .","title":"Workspaces"},{"location":"workspaces/#workspaces","text":"class api.models.workspace. Workspace ( *args , **kwargs ) A Workspace is a way to logically group the files and and analyses that are part of a user's work. Users can have multiple Workspace s to separate distinct analyses. Data, files, and analyses are grouped under a Workspace such that all information related to the analyses, including analysis history, is captured the Workspace .","title":"Workspaces"}]}