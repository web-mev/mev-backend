{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the WebMEV documentation page. This will serve as the main source of information and documentation on the architecture and structure of MEV and its RESTful API. For documentation on the endpoints, checkout the API specification","title":"Home"},{"location":"#welcome-to-the-webmev-documentation-page","text":"This will serve as the main source of information and documentation on the architecture and structure of MEV and its RESTful API. For documentation on the endpoints, checkout the API specification","title":"Welcome to the WebMEV documentation page."},{"location":"api/","text":"Documentation on the API There are a couple aspects to the WevMEV REST API. We have the actual API endpoints which are used to drive analysis or a frontend visualization. Documentation of the API interaction is provided by auto-generated documentation conforming to the OpenAPI spec here: API documentation Behind the API itself are the data structures and models that we use to architect the system. You will find information about these entities and their relationships in this section.","title":"Intro"},{"location":"api/#documentation-on-the-api","text":"There are a couple aspects to the WevMEV REST API. We have the actual API endpoints which are used to drive analysis or a frontend visualization. Documentation of the API interaction is provided by auto-generated documentation conforming to the OpenAPI spec here: API documentation Behind the API itself are the data structures and models that we use to architect the system. You will find information about these entities and their relationships in this section.","title":"Documentation on the API"},{"location":"attributes/","text":"Attributes Attribute s could also be thought of as \"parameters\" and are a way of providing validated key-value pairs. The different types enforce various constraints on the underlying primitive type (e.g. a float bounded between [0,1] can represent a probability). Mainly, Attribute s are a way to add information to Observation or Feature instances. For example, one could specify the phenotype or experimental group of an Observation via a StringAttribute . class api.data_structures.attributes. BaseAttribute ( value , **kwargs ) Base object which defines some common methods and members for Attribute types Classes that derive from BaseAttribute have strings which identify their type ( typename ) and a value , which is specific to the child class implementation. See child classes for examples. class api.data_structures.attributes. BoundedBaseAttribute ( value , **kwargs ) This class derives from BaseAttribute and adds logic for numeric attributes that are bounded between specified values. In addition to the typename and value members, these require a min and a max to set the bounds. Classes deriving from this can be used for things like bounding a p-value from a hypothesis test (which is 0<=p<=1) class api.data_structures.attributes. IntegerAttribute ( value , **kwargs ) General, unbounded integers. Represented by { \"attribute_type\": \"Integer\", \"value\": <integer> } class api.data_structures.attributes. PositiveIntegerAttribute ( value , **kwargs ) Integers > 0 { \"attribute_type\": \"PositiveInteger\", \"value\": <integer> } class api.data_structures.attributes. NonnegativeIntegerAttribute ( value , **kwargs ) Integers >=0 { \"attribute_type\": \"NonNegativeInteger\", \"value\": <integer> } class api.data_structures.attributes. BoundedIntegerAttribute ( value , **kwargs ) Integers that are bounded between a min and max value. { \"attribute_type\": \"BoundedInteger\", \"value\": <integer>, \"min\": <integer lower bound>, \"max\": <integer upper bound> } class api.data_structures.attributes. FloatAttribute ( value , **kwargs ) General, unbounded float type { \"attribute_type\": \"Float\", \"value\": <float> } class api.data_structures.attributes. PositiveFloatAttribute ( value , **kwargs ) Positive (>0) float type { \"attribute_type\": \"PositiveFloat\", \"value\": <float> } class api.data_structures.attributes. NonnegativeFloatAttribute ( value , **kwargs ) Non-negative (>=0) float type { \"attribute_type\": \"NonNegativeFloat\", \"value\": <float> } class api.data_structures.attributes. BoundedFloatAttribute ( value , **kwargs ) Floats that are bounded between a min and max value. { \"attribute_type\": \"BoundedFloat\", \"value\": <float>, \"min\": <integer/float lower bound>, \"max\": <integer/float upper bound> } class api.data_structures.attributes. StringAttribute ( value , **kwargs ) String type that has basic guards against non-typical characters. { \"attribute_type\": \"String\", \"value\": <str> } api.data_structures.attributes. create_attribute ( attr_key , attribute_dict ) Utility function used by the serializers to create/return BaseAttribute-derived instances. Accepts an attribute_dict which is a Python dictionary object containing the keys appropriate to create a particular attribute. For example, to create a BoundedIntegerAttribute , this dict would be formatted as, attr_dict = { 'attribute_type': 'BoundedInteger', 'value': 3, 'min': 0, 'max': 10 }","title":"Attributes"},{"location":"attributes/#attributes","text":"Attribute s could also be thought of as \"parameters\" and are a way of providing validated key-value pairs. The different types enforce various constraints on the underlying primitive type (e.g. a float bounded between [0,1] can represent a probability). Mainly, Attribute s are a way to add information to Observation or Feature instances. For example, one could specify the phenotype or experimental group of an Observation via a StringAttribute . class api.data_structures.attributes. BaseAttribute ( value , **kwargs ) Base object which defines some common methods and members for Attribute types Classes that derive from BaseAttribute have strings which identify their type ( typename ) and a value , which is specific to the child class implementation. See child classes for examples. class api.data_structures.attributes. BoundedBaseAttribute ( value , **kwargs ) This class derives from BaseAttribute and adds logic for numeric attributes that are bounded between specified values. In addition to the typename and value members, these require a min and a max to set the bounds. Classes deriving from this can be used for things like bounding a p-value from a hypothesis test (which is 0<=p<=1) class api.data_structures.attributes. IntegerAttribute ( value , **kwargs ) General, unbounded integers. Represented by { \"attribute_type\": \"Integer\", \"value\": <integer> } class api.data_structures.attributes. PositiveIntegerAttribute ( value , **kwargs ) Integers > 0 { \"attribute_type\": \"PositiveInteger\", \"value\": <integer> } class api.data_structures.attributes. NonnegativeIntegerAttribute ( value , **kwargs ) Integers >=0 { \"attribute_type\": \"NonNegativeInteger\", \"value\": <integer> } class api.data_structures.attributes. BoundedIntegerAttribute ( value , **kwargs ) Integers that are bounded between a min and max value. { \"attribute_type\": \"BoundedInteger\", \"value\": <integer>, \"min\": <integer lower bound>, \"max\": <integer upper bound> } class api.data_structures.attributes. FloatAttribute ( value , **kwargs ) General, unbounded float type { \"attribute_type\": \"Float\", \"value\": <float> } class api.data_structures.attributes. PositiveFloatAttribute ( value , **kwargs ) Positive (>0) float type { \"attribute_type\": \"PositiveFloat\", \"value\": <float> } class api.data_structures.attributes. NonnegativeFloatAttribute ( value , **kwargs ) Non-negative (>=0) float type { \"attribute_type\": \"NonNegativeFloat\", \"value\": <float> } class api.data_structures.attributes. BoundedFloatAttribute ( value , **kwargs ) Floats that are bounded between a min and max value. { \"attribute_type\": \"BoundedFloat\", \"value\": <float>, \"min\": <integer/float lower bound>, \"max\": <integer/float upper bound> } class api.data_structures.attributes. StringAttribute ( value , **kwargs ) String type that has basic guards against non-typical characters. { \"attribute_type\": \"String\", \"value\": <str> } api.data_structures.attributes. create_attribute ( attr_key , attribute_dict ) Utility function used by the serializers to create/return BaseAttribute-derived instances. Accepts an attribute_dict which is a Python dictionary object containing the keys appropriate to create a particular attribute. For example, to create a BoundedIntegerAttribute , this dict would be formatted as, attr_dict = { 'attribute_type': 'BoundedInteger', 'value': 3, 'min': 0, 'max': 10 }","title":"Attributes"},{"location":"auth/","text":"Authentication with MEV Once a user is registered (with an email and password), requests to the API are controlled with a JWT contained in the request header. Below is an example using Python's Requests library. This example assumes you have created a user. First, exchange the username/password to get the API token: import requests token_url = 'http://127.0.0.1:8000/api/token/' payload = {'email': '<EMAIL>', 'password': '<PASSWD>'} token_response = requests.post(token_url, data=payload) token_json = token_response.json() Then, looking at token_json : {'refresh': '<REFRESH TOKEN>', 'access': '<ACCESS_TOKEN>'} We can then use that token in requests to the API: access_token = token_json['access'] resource_list_url = 'http://127.0.0.1:8000/api/resources/' headers = {'Authorization': 'Bearer %s' % access_token} resource_response = requests.get(resource_list_url, headers=headers) resource_json = resource_response.json() If the token expires (a 401 response), you need to request a new token or refresh: refresh_url = 'http://127.0.0.1:8000/api/token/refresh/' payload = {'refresh': refresh_token} refresh_response = requests.post(refresh_url, data=payload) access_token = refresh_response.json()['access']","title":"Authentication"},{"location":"auth/#authentication-with-mev","text":"Once a user is registered (with an email and password), requests to the API are controlled with a JWT contained in the request header. Below is an example using Python's Requests library. This example assumes you have created a user. First, exchange the username/password to get the API token: import requests token_url = 'http://127.0.0.1:8000/api/token/' payload = {'email': '<EMAIL>', 'password': '<PASSWD>'} token_response = requests.post(token_url, data=payload) token_json = token_response.json() Then, looking at token_json : {'refresh': '<REFRESH TOKEN>', 'access': '<ACCESS_TOKEN>'} We can then use that token in requests to the API: access_token = token_json['access'] resource_list_url = 'http://127.0.0.1:8000/api/resources/' headers = {'Authorization': 'Bearer %s' % access_token} resource_response = requests.get(resource_list_url, headers=headers) resource_json = resource_response.json() If the token expires (a 401 response), you need to request a new token or refresh: refresh_url = 'http://127.0.0.1:8000/api/token/refresh/' payload = {'refresh': refresh_token} refresh_response = requests.post(refresh_url, data=payload) access_token = refresh_response.json()['access']","title":"Authentication with MEV"},{"location":"elements/","text":"Elements, Observations, and Features We adopt the convention from statistical learning of referring to Observation s and Feature s of data. Both of these data structures derive from BaseElement , which captures their common structure. Specialization specific to each can be overridden in the child classes. In an experimental context, Observation s are analogous to samples. That is, each Observation has one or more Feature s associated with it (e.g. gene expressions for 30,000 genes). Collectively, we can think of Observation s and Feature s as comprising the rows and columns of a two-dimensional matrix. We use Observation s and Feature s to hold metadata about data that we manipulating in MEV. We can attach attributes to these to allow users to set experimental groups, or other information usedful for visualization or filtering. These data structures have similar (if not exactly the same) behavior but we separate them for future compatability in case specialization of each class is needed. class api.data_structures.element. BaseElement ( id , attribute_dict={} ) A BaseElement is a base class from which we can derive both Observation and Features . For the purposes of clarity, we keep those entities separate. Yet, their behavior and structure are very much the same. This also allows us to add custom behavior to each at a later time if we require. An Element is structured as: { \"id\": <string identifier>, \"attributes\": { \"keyA\": <Attribute>, \"keyB\": <Attribute> } } class api.data_structures.observation. Observation ( id , attribute_dict={} ) An Observation is the generalization of a \"sample\" in the typical context of biological studies. One may think of samples and observations as interchangeable concepts. We call it an observation so that we are not limited by this convention, however. Observation instances act as metadata and can be used to filter and subset the data to which it is associated/attached. An Observation is structured as: { \"id\": <string identifier>, \"attributes\": { \"keyA\": <Attribute>, \"keyB\": <Attribute> } } class api.data_structures.feature. Feature ( id , attribute_dict={} ) A Feature can also be referred to as a covariate or variable. These are measurements one can make about an Observation . For example, in the genomics context, a sample can have 30,000+ genes which we call \"features\" here. In the statistical learning context, these are feature vectors. Feature instances act as metadata and can be used to filter and subset the data to which it is associated/attached. For example, we can imagine filtering by genes/features which have a particular value, such as those genes where the attribute \"oncogene\" is set to \"true\" A Feature is structured as: { \"id\": <string identifier>, \"attributes\": { \"keyA\": <Attribute>, \"keyB\": <Attribute> } }","title":"Observations and Features"},{"location":"elements/#elements-observations-and-features","text":"We adopt the convention from statistical learning of referring to Observation s and Feature s of data. Both of these data structures derive from BaseElement , which captures their common structure. Specialization specific to each can be overridden in the child classes. In an experimental context, Observation s are analogous to samples. That is, each Observation has one or more Feature s associated with it (e.g. gene expressions for 30,000 genes). Collectively, we can think of Observation s and Feature s as comprising the rows and columns of a two-dimensional matrix. We use Observation s and Feature s to hold metadata about data that we manipulating in MEV. We can attach attributes to these to allow users to set experimental groups, or other information usedful for visualization or filtering. These data structures have similar (if not exactly the same) behavior but we separate them for future compatability in case specialization of each class is needed. class api.data_structures.element. BaseElement ( id , attribute_dict={} ) A BaseElement is a base class from which we can derive both Observation and Features . For the purposes of clarity, we keep those entities separate. Yet, their behavior and structure are very much the same. This also allows us to add custom behavior to each at a later time if we require. An Element is structured as: { \"id\": <string identifier>, \"attributes\": { \"keyA\": <Attribute>, \"keyB\": <Attribute> } } class api.data_structures.observation. Observation ( id , attribute_dict={} ) An Observation is the generalization of a \"sample\" in the typical context of biological studies. One may think of samples and observations as interchangeable concepts. We call it an observation so that we are not limited by this convention, however. Observation instances act as metadata and can be used to filter and subset the data to which it is associated/attached. An Observation is structured as: { \"id\": <string identifier>, \"attributes\": { \"keyA\": <Attribute>, \"keyB\": <Attribute> } } class api.data_structures.feature. Feature ( id , attribute_dict={} ) A Feature can also be referred to as a covariate or variable. These are measurements one can make about an Observation . For example, in the genomics context, a sample can have 30,000+ genes which we call \"features\" here. In the statistical learning context, these are feature vectors. Feature instances act as metadata and can be used to filter and subset the data to which it is associated/attached. For example, we can imagine filtering by genes/features which have a particular value, such as those genes where the attribute \"oncogene\" is set to \"true\" A Feature is structured as: { \"id\": <string identifier>, \"attributes\": { \"keyA\": <Attribute>, \"keyB\": <Attribute> } }","title":"Elements, Observations, and Features"},{"location":"example_workflow/","text":"Example workflow To demonstrate how the various components of MEV come together, an graphical depiction of a typical workflow is shown below. The steps will be discussed in detail and connected to the various entities of the MEV architecture. Associated with each DataResource (AKA a file, depicted as rectangles above) is an ObservationSet , a FeatureSet , neither, or both. ObservationSet and FeatureSet s are essentially indexes on the columns/samples and the rows/genes as explained in Resource metadata . Step-by-step Denote the samples/columns of the original matrix (an instance of ObservationSet ) as all_observations . Similarly, denote all the rows/genes (an instance of FeatureSet ) as all_features . The original count matrix is run through the \"standard\"/automatic analyses. These are depicted using the gears and each are instances of Operation s. An Operation is essentially a function-- it has some input(s) and produces some output(s). Each of those Operation instances creates some output data/files. The content/format of those is not important here. Depending on the Operation , outputs could be flat files stored server-side (and served to the client) or simply data structures served to the client. One of those Operation s (PCA) allows you to create a \"selection\", which amounts to selecting a subset of all the samples. This is shown at point \"A\" in the figure. Through the UI, the user selects the desired samples (e.g. by clicking on points or dragging over areas of the PCA plot) and implicitly creates a client-side ObservationSet , which we will call pca_based_filter . This pca_based_filter is necessarily a subset of all_observations . Note that the user does not know about the concept of ObservationSet instances. Rather, they are simply selecting samples and choosing to group and label them according to some criteria (e.g. being clustered in a PCA plot). Also note that the dotted line in the figure is meant to suggest that pca_based_filter was \"inspired by\" by the PCA Operation , but did not actually derive from it. That is, while the visuals of the PCA plot were used to create the filter, the actual data of the PCA (the projected points) is not part of pca_based_filter (which is an ObservationSet ). Users can, however, name the ObservationSet so that they can be reminded of how these \"selections\" were made. At point \"B\", we apply that pca_based_filter to filter the columns of the original count matrix (recall that the columns of that original file is all_observations ). Although the icon is not a \"gear\", the green circle denoting the application of the filter is still an Operation in the MEV context. Also, note that we can apply the pca_based_filter filter to any existing file that has an ObservationSet . Obviously, it only provides a useful filter if there is a non-empty intersection of those sets; otherwise the filter produces an empty result. That is technically a valid Operation , however. At this point, the only existing DataResource /file is the original count matrix which has an ObservationSet we called all_observations . and we certainly have a non-empty intersection of the sets pca_based_filter and all_observations , so the filter is \"useful\". In accordance with our notion of an Observation , the filtering takes inputs (an ObservationSet and a DataResource ) and produces output(s); here the output is another DataResource which we call matrixA . In the backend, this creates both a physical file and a Resource in the database. Recall that a Resource is just a way for MEV to track files, but is agnostic of the \"behavior\" of the files. We next run a differential expression analysis (DESeq2) on matrixA . This produces a table of differential expression results. Note that when we choose to run DESeq2, we will be given the option of choosing from all available count matrices. In our case, that is either the original count matrix or the matrixA . We choose matrixA in the diagram. At point \"C\", we create a \"row filter\" by selecting the significantly differentially expressed genes from the DESeq2 results table. Recall that in our nomenclature we call this a FeatureSet . This FeatureSet (name it dge_fs ) can then be applied to any of the existing files where it makes sense. Again, by that I mean that it can be applied as a filter to any existing table that has a FeatureSet . Currently those are: original count matrix (where we called it all_features ) matrixA DESEq2 table Since we have not yet applied any row filters, all three of those DataResource s/files have FeatureSet s equivalent to all_features . The three files are shown flowing into node \"D\", but only one can be chosen (shown with solid line- matrixA ) At point \"D\", we apply dge_fs to matrixA in a filtering Operation . This produces a new file which we call matrixB . If you're keeping score, matrixB is basically the original table with both a row and column filter applied. We then run analyses on matrixB , such as a new PCA and a GSEA analysis. Additional notes This way of operation ends up producing multiple files that copy portions of the original matrix. We could try and be slick and store those filter operations, but it's easier to just write new files. Allowing multiple DataResource s/files within a Workspace allows us to use multiple sources of data within an analysis. In the older iterations MEV, all the analyses have to \"flow\" from a single original file. This is more or less what we did in the figure above, but we are no longer constrained to operate in that way. One could imagine adding a VCF file to the Workspace which might allow one to perform an eQTL analysis, for example.","title":"Workflow and analysis concepts"},{"location":"example_workflow/#example-workflow","text":"To demonstrate how the various components of MEV come together, an graphical depiction of a typical workflow is shown below. The steps will be discussed in detail and connected to the various entities of the MEV architecture. Associated with each DataResource (AKA a file, depicted as rectangles above) is an ObservationSet , a FeatureSet , neither, or both. ObservationSet and FeatureSet s are essentially indexes on the columns/samples and the rows/genes as explained in Resource metadata . Step-by-step Denote the samples/columns of the original matrix (an instance of ObservationSet ) as all_observations . Similarly, denote all the rows/genes (an instance of FeatureSet ) as all_features . The original count matrix is run through the \"standard\"/automatic analyses. These are depicted using the gears and each are instances of Operation s. An Operation is essentially a function-- it has some input(s) and produces some output(s). Each of those Operation instances creates some output data/files. The content/format of those is not important here. Depending on the Operation , outputs could be flat files stored server-side (and served to the client) or simply data structures served to the client. One of those Operation s (PCA) allows you to create a \"selection\", which amounts to selecting a subset of all the samples. This is shown at point \"A\" in the figure. Through the UI, the user selects the desired samples (e.g. by clicking on points or dragging over areas of the PCA plot) and implicitly creates a client-side ObservationSet , which we will call pca_based_filter . This pca_based_filter is necessarily a subset of all_observations . Note that the user does not know about the concept of ObservationSet instances. Rather, they are simply selecting samples and choosing to group and label them according to some criteria (e.g. being clustered in a PCA plot). Also note that the dotted line in the figure is meant to suggest that pca_based_filter was \"inspired by\" by the PCA Operation , but did not actually derive from it. That is, while the visuals of the PCA plot were used to create the filter, the actual data of the PCA (the projected points) is not part of pca_based_filter (which is an ObservationSet ). Users can, however, name the ObservationSet so that they can be reminded of how these \"selections\" were made. At point \"B\", we apply that pca_based_filter to filter the columns of the original count matrix (recall that the columns of that original file is all_observations ). Although the icon is not a \"gear\", the green circle denoting the application of the filter is still an Operation in the MEV context. Also, note that we can apply the pca_based_filter filter to any existing file that has an ObservationSet . Obviously, it only provides a useful filter if there is a non-empty intersection of those sets; otherwise the filter produces an empty result. That is technically a valid Operation , however. At this point, the only existing DataResource /file is the original count matrix which has an ObservationSet we called all_observations . and we certainly have a non-empty intersection of the sets pca_based_filter and all_observations , so the filter is \"useful\". In accordance with our notion of an Observation , the filtering takes inputs (an ObservationSet and a DataResource ) and produces output(s); here the output is another DataResource which we call matrixA . In the backend, this creates both a physical file and a Resource in the database. Recall that a Resource is just a way for MEV to track files, but is agnostic of the \"behavior\" of the files. We next run a differential expression analysis (DESeq2) on matrixA . This produces a table of differential expression results. Note that when we choose to run DESeq2, we will be given the option of choosing from all available count matrices. In our case, that is either the original count matrix or the matrixA . We choose matrixA in the diagram. At point \"C\", we create a \"row filter\" by selecting the significantly differentially expressed genes from the DESeq2 results table. Recall that in our nomenclature we call this a FeatureSet . This FeatureSet (name it dge_fs ) can then be applied to any of the existing files where it makes sense. Again, by that I mean that it can be applied as a filter to any existing table that has a FeatureSet . Currently those are: original count matrix (where we called it all_features ) matrixA DESEq2 table Since we have not yet applied any row filters, all three of those DataResource s/files have FeatureSet s equivalent to all_features . The three files are shown flowing into node \"D\", but only one can be chosen (shown with solid line- matrixA ) At point \"D\", we apply dge_fs to matrixA in a filtering Operation . This produces a new file which we call matrixB . If you're keeping score, matrixB is basically the original table with both a row and column filter applied. We then run analyses on matrixB , such as a new PCA and a GSEA analysis. Additional notes This way of operation ends up producing multiple files that copy portions of the original matrix. We could try and be slick and store those filter operations, but it's easier to just write new files. Allowing multiple DataResource s/files within a Workspace allows us to use multiple sources of data within an analysis. In the older iterations MEV, all the analyses have to \"flow\" from a single original file. This is more or less what we did in the figure above, but we are no longer constrained to operate in that way. One could imagine adding a VCF file to the Workspace which might allow one to perform an eQTL analysis, for example.","title":"Example workflow"},{"location":"install/","text":"Installation instructions Dockerhub image TODO Local build The application is packaged as a series of Docker images for easier management of dependencies. The collective behavior and interdependencies is managed by docker-compose . To build the application image yourself, you will need to have Docker and docker-compose installed locally. To build the application, clone the repository locally: git clone https://github.com/web-mev/mev-backend.git To run the container, you will need to supply some environment variables for things like passwords and other sensitive information. In the root of the repository is a file named env_vars.template.txt . Fill that in with the various usernames and passwords as you like. Each variable has a description to help. In particular, note the following: DJANGO_ALLOWED_HOSTS : If you are deploying on a host that is not your own computer (e.g. on a cloud machine), supply the IP address or domain. DJANGO_CORS_ORIGINS : If you are using a front-end framework located on a different domain, you will need to add this to the comma-delimited list. Otherwise you will get failures due to violoating same-origin policy. RESOURCE_STORAGE_BACKEND : This configures where user files are stored. By default, it is local. Other storage backends may be configured (i.e. for cloud bucket-based storage). Given as a \"dotted\" path to the storage class implementation. EMAIL_BACKEND_CHOICE : Email-based registration depends on the ability to send emails, so a viable email backend must be chosen. Refer to the settings file to see available options. Currently only GMAIL . SOCIAL_BACKENDS : A list of social auth providers you wish to use. Currently only Google. If you would like some \"dummy\" data to be entered into the database (e.g. for developing a front-end), you must also specify POPULUATE_DB=yes (case-sensitive!). Any other values for this variable will skip the population of dummy data. For more information on configuration, see Configuration To start the application in development mode: In development mode, the application server (gunicorn) does not start automatically as the container starts. Rather, all the containers are started but the application container ( api ) remains idle. This allows us to mount a local directory where we can dynamically edit the code and immediately see changes. Note that the Django DEBUG argument is set to True in this case, so be mindful of using development mode if the server is exposed to the public. In the root of the repository (where the docker-compose.yml file resides) run: docker-compose up -d --build This builds all the containers and runs in \"detached\" mode. By default, docker-compose will look for the docker-compose.yml file which, by design, puts us in development mode. The current directory on the host machine will be mounted at /workspace inside the api container. Next, enter the container with: docker-compose exec api /bin/bash You may optionally choose to add the -u 0 flag which logs you into the api container as root. Once inside, run the following: cd /workspace/mev source startup.sh This will run some database migrations and other preliminaries, but will not actually start the gunicorn application server. To do that, you must then run: cd /workspace/mev gunicorn mev.wsgi:application --bind 0.0.0.0:8000 (note the cd at the top since the startup.sh script ends up moving you into the /www directory). The bind argument should have the port set to 8000 as this is how the NGINX container communicates over the internal docker-compose network. Following all that, the application should be running on port 8081 (e.g. http://127.0.0.1:8081/api/) If you are interested, note that additional gunicorn configuration parameters can be specified (see https://docs.gunicorn.org/en/latest/configure.html). By stopping the gunicorn server (Ctrl+C), you can make local edits to your code (again, connected into the container via the volume mount) and immediately restart the server to see the changes. The unit test suite can also be run in this manner with python3 /workspace/mev/manage.py test To start the application in production mode: In production mode, the application server will be started following the usual startup steps contained in startup.sh . In the root of the repository (where the docker-compose.prod.yml file resides) run: docker-compose -f docker-compose.prod.yml up -d --build This should start everything up. On occasion, if you are very quick to navigate to the site, NGINX will issue a 502 bad gateway error. However, a refresh should open the site correctly. In production mode, Django debugging is turned off, so any errors will be reported as generic 500 server errors without any corresponding details. Stopping the application To shut down the application (in verbose mode), run docker-compose down -v","title":"Installation"},{"location":"install/#installation-instructions","text":"","title":"Installation instructions"},{"location":"install/#dockerhub-image","text":"TODO","title":"Dockerhub image"},{"location":"install/#local-build","text":"The application is packaged as a series of Docker images for easier management of dependencies. The collective behavior and interdependencies is managed by docker-compose . To build the application image yourself, you will need to have Docker and docker-compose installed locally. To build the application, clone the repository locally: git clone https://github.com/web-mev/mev-backend.git To run the container, you will need to supply some environment variables for things like passwords and other sensitive information. In the root of the repository is a file named env_vars.template.txt . Fill that in with the various usernames and passwords as you like. Each variable has a description to help. In particular, note the following: DJANGO_ALLOWED_HOSTS : If you are deploying on a host that is not your own computer (e.g. on a cloud machine), supply the IP address or domain. DJANGO_CORS_ORIGINS : If you are using a front-end framework located on a different domain, you will need to add this to the comma-delimited list. Otherwise you will get failures due to violoating same-origin policy. RESOURCE_STORAGE_BACKEND : This configures where user files are stored. By default, it is local. Other storage backends may be configured (i.e. for cloud bucket-based storage). Given as a \"dotted\" path to the storage class implementation. EMAIL_BACKEND_CHOICE : Email-based registration depends on the ability to send emails, so a viable email backend must be chosen. Refer to the settings file to see available options. Currently only GMAIL . SOCIAL_BACKENDS : A list of social auth providers you wish to use. Currently only Google. If you would like some \"dummy\" data to be entered into the database (e.g. for developing a front-end), you must also specify POPULUATE_DB=yes (case-sensitive!). Any other values for this variable will skip the population of dummy data. For more information on configuration, see Configuration To start the application in development mode: In development mode, the application server (gunicorn) does not start automatically as the container starts. Rather, all the containers are started but the application container ( api ) remains idle. This allows us to mount a local directory where we can dynamically edit the code and immediately see changes. Note that the Django DEBUG argument is set to True in this case, so be mindful of using development mode if the server is exposed to the public. In the root of the repository (where the docker-compose.yml file resides) run: docker-compose up -d --build This builds all the containers and runs in \"detached\" mode. By default, docker-compose will look for the docker-compose.yml file which, by design, puts us in development mode. The current directory on the host machine will be mounted at /workspace inside the api container. Next, enter the container with: docker-compose exec api /bin/bash You may optionally choose to add the -u 0 flag which logs you into the api container as root. Once inside, run the following: cd /workspace/mev source startup.sh This will run some database migrations and other preliminaries, but will not actually start the gunicorn application server. To do that, you must then run: cd /workspace/mev gunicorn mev.wsgi:application --bind 0.0.0.0:8000 (note the cd at the top since the startup.sh script ends up moving you into the /www directory). The bind argument should have the port set to 8000 as this is how the NGINX container communicates over the internal docker-compose network. Following all that, the application should be running on port 8081 (e.g. http://127.0.0.1:8081/api/) If you are interested, note that additional gunicorn configuration parameters can be specified (see https://docs.gunicorn.org/en/latest/configure.html). By stopping the gunicorn server (Ctrl+C), you can make local edits to your code (again, connected into the container via the volume mount) and immediately restart the server to see the changes. The unit test suite can also be run in this manner with python3 /workspace/mev/manage.py test To start the application in production mode: In production mode, the application server will be started following the usual startup steps contained in startup.sh . In the root of the repository (where the docker-compose.prod.yml file resides) run: docker-compose -f docker-compose.prod.yml up -d --build This should start everything up. On occasion, if you are very quick to navigate to the site, NGINX will issue a 502 bad gateway error. However, a refresh should open the site correctly. In production mode, Django debugging is turned off, so any errors will be reported as generic 500 server errors without any corresponding details. Stopping the application To shut down the application (in verbose mode), run docker-compose down -v","title":"Local build"},{"location":"operations/","text":"Operations and ExecutedOperations An Operation is any manipulation of some data that produces some output; it defines the type of analysis that is run, its inputs and outputs, and other relevant information. An Operation can be as simple as selecting a subset of the columns/rows of a matrix or running a large-scale processing job that spans many machines and significant time. An ExecutedOperation represents an actual execution of an Operation . While the Operation identifies the process used, the ExecutedOperation contains information about the actual execution, such as the job status, the exact inputs and outputs, the runtime, and other relevant information. Clearly, the ExecutedOperation maintains a foreign-key relation to the Operation . The various ExecutedOperation s performed in MEV will all create some output so that there will be no ambiguity regarding how data was manipulated through the course of an analysis workflow. Essentially, we do not perform in-place operations on data. For example, if a user chooses a subset of samples/columns in their expression matrix, we create a new DataResource (and corresponding Resource database record). While this has the potential to create multiple files with similar data, this makes auditing a workflow history much simpler. Typically, the size of files where users are interacting with the data are relatively small (on the order of MB) so excessive storage is not a major concern. Note that client-side manipulations of the data, such as filtering out samples are distinct from this concept of an Operation / ExecutedOperation . That is, users can select various filters on their data to change the visualizations without executing any Operation s. However , once they choose to use the subset data for use in an analysis, they will be required to implicitly execute an Operation on the backend. As a concrete example, consider analyzing a large cohort of expression data from TCGA. A user initially imports the expression matrix and perhaps uses PCA or some other clustering method in an exploratory analysis. Each of those initial analyses were ExecutedOperation s. Based on those initial visualizations, the user may select a subset of those samples to investigate for potential subtypes; note that those client-side sample selections have not triggered any actual analyses. However, once they choose to run those samples through a differential expression analysis, we require that they perform filter/subset Operation . As new DataResource s are created, the metadata tracks which ExecutedOperation created them, addressed by the UUID assigned to each ExecutedOperation . By tracing the foreign-key relation, we can determine the exact Operation that was run so that the steps of the analysis are transparent and reproducible. Operation s can be lightweight jobs such as a basic filter or a simple R script, or involve complex, multi-step pipelines orchestrated using the CNAP-style workflows involving WDL and Cromwell. Depending on the computational resources needed, the Operation can be run locally or remotely. As jobs complete, their outputs will populate in the user's workspace and further analysis can be performed. All jobs, whether local or remote, will be placed in a queue and executed ascynchronously. Progress/status of remote jobs can be monitored by querying the Cromwell server. Also note that ALL Operation s (even basic table filtering) are executed in Docker containers so that the software and environments can be carefully tracked and controlled. This ensures a consistent \"plugin\" style architecture so that new Operation s can be integrated consistently. Operation s should maintain the following data: unique identifier (UUID) name description of the analysis Inputs to the analysis. These are the acceptable types and potentially some parameters. For instance, a DESeq2 analysis should take an IntegerMatrix , two ObservationSet instances (defining the contrast groups), and a string \"name\" for the contrast. See below for a concrete example. Outputs of the analysis. This would be similar to the inputs in that it describes the \"types\" of outputs. Again, using the DESEq2 example, the outputs could be a \"feature table\" ( FT , FeatureTable ) giving the table of differentially expressed genes (e.g. fold-change, p-values) and a normalized expression matrix of type \"expression matrix\" ( EXP_MTX ). github repo/Docker info (commit hashes) so that the backend analysis code may be traced whether the Operation is a \"local\" one or requires use of the Cromwell engine. The front-end users don't need to know that, but internally MEV needs to know how to run the analysis. Note that some of these will be specified by whoever creates the analysis. However, some information (like the UUID identifier, git hash, etc.) will be populated when the analysis is \"ingested\". It should not be the job of the analysis developer to maintain those pieces of information and we can create them on the fly during ingestion. Therefore, an Operation has the following structure: { \"id\": <UUID>, \"name\": <string>, \"description\": <string>, \"inputs\": Object<OperationInput>, \"outputs\": Object<OperationOutput>, \"mode\": <string>, \"repository\": <string url>, \"git_hash\": <string> } where: - mode : identifies how the analysis is run. Will be one of an enum of strings (e.g. local_docker , cromwell ) - repository : identifies the github repo used to pull the data. For the ingestion, admins will give that url which will initiate a clone process. - git_hash : This is the commit hash which uniquely identifies the code state. This way the analysis code can be exactly traced back. Both inputs and outputs address nested objects. For inputs, we have an OperationInput which looks like: { \"description\": <string>, \"name\": <string>, \"required\": <bool>, \"spec\": <InputSpec> } e.g. to specify a p-value for thresholding: { \"description\": \"The filtering threshold for the p-value\", \"name\": \"P-value threshold:\", \"required\": false, \"spec\": { \"type\": \"BoundedFloat\", \"min\": 0, \"max\": 1.0, \"default\": 0.05 } } The spec key addresses a child class of InputSpec whose behavior is specific to each \"type\" (above, a BoundedFloat ). There are only a limited number of those so defining a set of options for each is straightforward. Also note that the repository will contain the Dockerfile(s) needed for the analysis. In the case of local runs, we can actually build and push the containers during ingestion, tagging them appropriately in the process. For WDL-based processes, the docker tag (in the WDL runtime section) has to be set beforehand. So in that case the ingestion process will only check for the existence of the proper tagged image on Dockerhub. ExecutedOperation s should maintain the following data: The Workspace (which also gives access to the user/owner) a foreign-key to the Operation \"type\" unique identifier (UUID) for the execution a job identifier. We need the Cromwell job UUID to track the progress as we query the Cromwell server. For Docker-based jobs, we can set the tag on the container and then query its status (e.g. if it's still \"up\", then the job is still going) The inputs to the analysis (a JSON document) The outputs (another JSON document) once complete Job execution status (e.g. \"running\", \"complete\", \"failed\", etc.) Start time Completion time Any errors or warnings A concrete example For this, consider a differential expression analysis (e.g. like DESeq2). In this simplified analysis, we will take a count matrix, a p-value (for filtering significance based on some hypothesis test), and an output file name. For outputs, we have a single file which has the results of the differential expression testing on each gene. Since each row concerns a gene (and the columns give information about that gene), the output file is a \"feature table\" in our nomenclature. Thus, the Operation object could look like: { \"name\":\"DESeq2 differential gene expression\", \"description\": \"Find genes which are differentially expressed and filter...\" \"inputs\": { \"count_matrix\": { \"description\": \"The count matrix of expressions\", \"name\": \"Count matrix:\", \"required\": true, \"spec\": { \"attribute_type\": \"DataResource\", \"resource_types\": [\"RNASEQ_COUNT_MTX\", \"I_MTX\"], \"many\": false } }, \"p_val\": { \"description\": \"The filtering threshold for the p-value\", \"name\": \"P-value threshold:\", \"required\": false, \"spec\": { \"attribute_type\": \"BoundedFloat\", \"min\": 0, \"max\": 1.0, \"default\": 0.05 }, } \"output_filename\": { \"description\": \"The name of the contrast for your own reference.\", \"name\": \"Contrast name:\", \"required\": false, \"spec\": { \"attribute_type\": \"String\", \"default\": \"deseq2_results\" } } }, \"outputs\": { \"dge_table\": { \"spec\":{ \"attribute_type\": \"DataResource\", \"resource_type\": \"FT\" } } } } This specification will be placed into a file. In the repo, there will be a Dockerfile and possibly other files (e.g. scripts). Upon ingestion, MEV will read this inputs file, get the commit hash, assign a UUID, build the container, push the container, etc. The UUID will be placed into the database, and the repository files will be saved into a folder. The UUID will allow MEV to locate the appropriate files when needed.","title":"Operations"},{"location":"operations/#operations-and-executedoperations","text":"An Operation is any manipulation of some data that produces some output; it defines the type of analysis that is run, its inputs and outputs, and other relevant information. An Operation can be as simple as selecting a subset of the columns/rows of a matrix or running a large-scale processing job that spans many machines and significant time. An ExecutedOperation represents an actual execution of an Operation . While the Operation identifies the process used, the ExecutedOperation contains information about the actual execution, such as the job status, the exact inputs and outputs, the runtime, and other relevant information. Clearly, the ExecutedOperation maintains a foreign-key relation to the Operation . The various ExecutedOperation s performed in MEV will all create some output so that there will be no ambiguity regarding how data was manipulated through the course of an analysis workflow. Essentially, we do not perform in-place operations on data. For example, if a user chooses a subset of samples/columns in their expression matrix, we create a new DataResource (and corresponding Resource database record). While this has the potential to create multiple files with similar data, this makes auditing a workflow history much simpler. Typically, the size of files where users are interacting with the data are relatively small (on the order of MB) so excessive storage is not a major concern. Note that client-side manipulations of the data, such as filtering out samples are distinct from this concept of an Operation / ExecutedOperation . That is, users can select various filters on their data to change the visualizations without executing any Operation s. However , once they choose to use the subset data for use in an analysis, they will be required to implicitly execute an Operation on the backend. As a concrete example, consider analyzing a large cohort of expression data from TCGA. A user initially imports the expression matrix and perhaps uses PCA or some other clustering method in an exploratory analysis. Each of those initial analyses were ExecutedOperation s. Based on those initial visualizations, the user may select a subset of those samples to investigate for potential subtypes; note that those client-side sample selections have not triggered any actual analyses. However, once they choose to run those samples through a differential expression analysis, we require that they perform filter/subset Operation . As new DataResource s are created, the metadata tracks which ExecutedOperation created them, addressed by the UUID assigned to each ExecutedOperation . By tracing the foreign-key relation, we can determine the exact Operation that was run so that the steps of the analysis are transparent and reproducible. Operation s can be lightweight jobs such as a basic filter or a simple R script, or involve complex, multi-step pipelines orchestrated using the CNAP-style workflows involving WDL and Cromwell. Depending on the computational resources needed, the Operation can be run locally or remotely. As jobs complete, their outputs will populate in the user's workspace and further analysis can be performed. All jobs, whether local or remote, will be placed in a queue and executed ascynchronously. Progress/status of remote jobs can be monitored by querying the Cromwell server. Also note that ALL Operation s (even basic table filtering) are executed in Docker containers so that the software and environments can be carefully tracked and controlled. This ensures a consistent \"plugin\" style architecture so that new Operation s can be integrated consistently. Operation s should maintain the following data: unique identifier (UUID) name description of the analysis Inputs to the analysis. These are the acceptable types and potentially some parameters. For instance, a DESeq2 analysis should take an IntegerMatrix , two ObservationSet instances (defining the contrast groups), and a string \"name\" for the contrast. See below for a concrete example. Outputs of the analysis. This would be similar to the inputs in that it describes the \"types\" of outputs. Again, using the DESEq2 example, the outputs could be a \"feature table\" ( FT , FeatureTable ) giving the table of differentially expressed genes (e.g. fold-change, p-values) and a normalized expression matrix of type \"expression matrix\" ( EXP_MTX ). github repo/Docker info (commit hashes) so that the backend analysis code may be traced whether the Operation is a \"local\" one or requires use of the Cromwell engine. The front-end users don't need to know that, but internally MEV needs to know how to run the analysis. Note that some of these will be specified by whoever creates the analysis. However, some information (like the UUID identifier, git hash, etc.) will be populated when the analysis is \"ingested\". It should not be the job of the analysis developer to maintain those pieces of information and we can create them on the fly during ingestion. Therefore, an Operation has the following structure: { \"id\": <UUID>, \"name\": <string>, \"description\": <string>, \"inputs\": Object<OperationInput>, \"outputs\": Object<OperationOutput>, \"mode\": <string>, \"repository\": <string url>, \"git_hash\": <string> } where: - mode : identifies how the analysis is run. Will be one of an enum of strings (e.g. local_docker , cromwell ) - repository : identifies the github repo used to pull the data. For the ingestion, admins will give that url which will initiate a clone process. - git_hash : This is the commit hash which uniquely identifies the code state. This way the analysis code can be exactly traced back. Both inputs and outputs address nested objects. For inputs, we have an OperationInput which looks like: { \"description\": <string>, \"name\": <string>, \"required\": <bool>, \"spec\": <InputSpec> } e.g. to specify a p-value for thresholding: { \"description\": \"The filtering threshold for the p-value\", \"name\": \"P-value threshold:\", \"required\": false, \"spec\": { \"type\": \"BoundedFloat\", \"min\": 0, \"max\": 1.0, \"default\": 0.05 } } The spec key addresses a child class of InputSpec whose behavior is specific to each \"type\" (above, a BoundedFloat ). There are only a limited number of those so defining a set of options for each is straightforward. Also note that the repository will contain the Dockerfile(s) needed for the analysis. In the case of local runs, we can actually build and push the containers during ingestion, tagging them appropriately in the process. For WDL-based processes, the docker tag (in the WDL runtime section) has to be set beforehand. So in that case the ingestion process will only check for the existence of the proper tagged image on Dockerhub. ExecutedOperation s should maintain the following data: The Workspace (which also gives access to the user/owner) a foreign-key to the Operation \"type\" unique identifier (UUID) for the execution a job identifier. We need the Cromwell job UUID to track the progress as we query the Cromwell server. For Docker-based jobs, we can set the tag on the container and then query its status (e.g. if it's still \"up\", then the job is still going) The inputs to the analysis (a JSON document) The outputs (another JSON document) once complete Job execution status (e.g. \"running\", \"complete\", \"failed\", etc.) Start time Completion time Any errors or warnings","title":"Operations and ExecutedOperations"},{"location":"operations/#a-concrete-example","text":"For this, consider a differential expression analysis (e.g. like DESeq2). In this simplified analysis, we will take a count matrix, a p-value (for filtering significance based on some hypothesis test), and an output file name. For outputs, we have a single file which has the results of the differential expression testing on each gene. Since each row concerns a gene (and the columns give information about that gene), the output file is a \"feature table\" in our nomenclature. Thus, the Operation object could look like: { \"name\":\"DESeq2 differential gene expression\", \"description\": \"Find genes which are differentially expressed and filter...\" \"inputs\": { \"count_matrix\": { \"description\": \"The count matrix of expressions\", \"name\": \"Count matrix:\", \"required\": true, \"spec\": { \"attribute_type\": \"DataResource\", \"resource_types\": [\"RNASEQ_COUNT_MTX\", \"I_MTX\"], \"many\": false } }, \"p_val\": { \"description\": \"The filtering threshold for the p-value\", \"name\": \"P-value threshold:\", \"required\": false, \"spec\": { \"attribute_type\": \"BoundedFloat\", \"min\": 0, \"max\": 1.0, \"default\": 0.05 }, } \"output_filename\": { \"description\": \"The name of the contrast for your own reference.\", \"name\": \"Contrast name:\", \"required\": false, \"spec\": { \"attribute_type\": \"String\", \"default\": \"deseq2_results\" } } }, \"outputs\": { \"dge_table\": { \"spec\":{ \"attribute_type\": \"DataResource\", \"resource_type\": \"FT\" } } } } This specification will be placed into a file. In the repo, there will be a Dockerfile and possibly other files (e.g. scripts). Upon ingestion, MEV will read this inputs file, get the commit hash, assign a UUID, build the container, push the container, etc. The UUID will be placed into the database, and the repository files will be saved into a folder. The UUID will allow MEV to locate the appropriate files when needed.","title":"A concrete example"},{"location":"resource_metadata/","text":"Resource metadata Metadata can be associated with type of DataResource . Note that a DataResource is related, but distinct from a Resource . The latter is for tracking the various file-based resources in the database; it knows about the file location, size, and the type of the resource (as a string field). The former is a base class from which the many specialized \"types\" of resources derive. For instance, an IntegerMatrix derives from a DataResource . Instead of being a database record, a DataResource captures the expected format and behavior of the resource. For instance, the children classes of DataResource contain validators and parsers. Thus, associated with each DataResource is some metadata. The specification may expand to incorporate additional fields, but at minimum, it should contain: An ObservationSet . For a FastQ file representing a single sample (most common case), the ObservationSet would have a single item (of type Observation ) containing information about that particular sample. For a count matrix of size (p, N), the ObservationSet would have N items (again, of type Observation ) giving information about the samples in the columns. A FeatureSet . This is a collection of covariates corresponding to a single Observation . A Feature is something that is measured (e.g. read counts for a gene). For a count matrix of size (p, N), the FeatureSet would have p items (of type Feature ) and correspond to the p genes measured for a single sample. For a sequence-based file like a FastQ, this would simply be null; perhaps there are alternative intepretations of this concept, but the point is that the field can be null. A table of differentially expressed genes would have a FeatureSet , but not an ObservationSet ; in this case the Feature s are the genes and we are given information like log-fold change and p-value. A parent operation. As an analysis workflow can be represented as a directed, acyclic graph (DAG), we would like to track the flow of data and operations on the data. Tracking the \"parent\" of a DataResource allows us to determine which operation generated the data and hence reconstruct the full DAG. The original input files would have a null parent. We maintain a \"master copy\" of the metadata on the server side as a flat file for reference. We do not want to have to repeatedly open/parse a large text file to determine the rows/features and columns/observations. We imagine that for performance reasons, client-applications may choose to cache this metadata so that desired sets of rows or columns can be selected on the client side without involving a request to the server. Requests to subset/filter a DataResource would provide ObservationSet s or FeatureSet s which are compared against the respective ObservationSet s or FeatureSet s of the DataResource .","title":"Resource metadata"},{"location":"resource_metadata/#resource-metadata","text":"Metadata can be associated with type of DataResource . Note that a DataResource is related, but distinct from a Resource . The latter is for tracking the various file-based resources in the database; it knows about the file location, size, and the type of the resource (as a string field). The former is a base class from which the many specialized \"types\" of resources derive. For instance, an IntegerMatrix derives from a DataResource . Instead of being a database record, a DataResource captures the expected format and behavior of the resource. For instance, the children classes of DataResource contain validators and parsers. Thus, associated with each DataResource is some metadata. The specification may expand to incorporate additional fields, but at minimum, it should contain: An ObservationSet . For a FastQ file representing a single sample (most common case), the ObservationSet would have a single item (of type Observation ) containing information about that particular sample. For a count matrix of size (p, N), the ObservationSet would have N items (again, of type Observation ) giving information about the samples in the columns. A FeatureSet . This is a collection of covariates corresponding to a single Observation . A Feature is something that is measured (e.g. read counts for a gene). For a count matrix of size (p, N), the FeatureSet would have p items (of type Feature ) and correspond to the p genes measured for a single sample. For a sequence-based file like a FastQ, this would simply be null; perhaps there are alternative intepretations of this concept, but the point is that the field can be null. A table of differentially expressed genes would have a FeatureSet , but not an ObservationSet ; in this case the Feature s are the genes and we are given information like log-fold change and p-value. A parent operation. As an analysis workflow can be represented as a directed, acyclic graph (DAG), we would like to track the flow of data and operations on the data. Tracking the \"parent\" of a DataResource allows us to determine which operation generated the data and hence reconstruct the full DAG. The original input files would have a null parent. We maintain a \"master copy\" of the metadata on the server side as a flat file for reference. We do not want to have to repeatedly open/parse a large text file to determine the rows/features and columns/observations. We imagine that for performance reasons, client-applications may choose to cache this metadata so that desired sets of rows or columns can be selected on the client side without involving a request to the server. Requests to subset/filter a DataResource would provide ObservationSet s or FeatureSet s which are compared against the respective ObservationSet s or FeatureSet s of the DataResource .","title":"Resource metadata"},{"location":"resource_types/","text":"Resource types A Resource represents some generic notion of data and its resource_type field/member is a string identifier that identifies the specific format of the data. The string identifiers map to concrete classes that implement validation methods for the Resource . When a new Resource is added (via upload or directly by an admin via the API), the validation method is called. Similarly, if a user tries to change the resource_type , it will trigger the validation process. Current resource_types fall into two broad categories: Table-based formats Sequence-based formats Table-based formats are any array-like format, such as a typical CSV file. This covers a wide variety of standard formats encountered in computational biology, including GTF annotation files and BED files. The primitive data types contained in each column are determined using Python's Pandas library, which refers to these as \"dtypes\"; for example, a column identified as int64 certainly qualifies as an integer type. If the column contains any non-integers (but all numbers), Pandas automatically converts it to a float type (e.g. float64 ) which allows us to easily validate the content of each column. Sequence-based formats are formats like FastQ, Fasta, or SAM/BAM. Table-based resource types class resource_types.table_types. TableResource ( ) The TableResource is the most generic form of a delimited file. Any type of data that can be represented as rows and columns. This or any of the more specific subclasses can be contained in files saved in CSV, TSV, or Excel (xls/xlsx) format. If in Excel format, the data of interest must reside in the first sheet of the workbook. Special tab-delimited files like BED or VCF files are recognized by their canonical extension (e.g. \".bed\" or \".vcf\"). Note that unless you create a \"specialized\" implementation (e.g. like for a BED file), then we assume you have features as rows and observables as columns. class resource_types.table_types. Matrix ( ) A Matrix is a delimited table-based file that has only numeric types. These types can be mixed, like floats and integers class resource_types.table_types. IntegerMatrix ( ) An IntegerMatrix further specializes the Matrix to admit only integers. class resource_types.table_types. AnnotationTable ( ) An AnnotationTable is a special type of table that will be responsible for annotating Observations/samples (e.g. adding sample names and associated attributes like experimental group or other covariates) The first column will give the sample names and the remaining columns will each individually represent different covariates associated with that sample. For example, if we received the following table: sample genotype treatment A WT Y B WT N Then this table can be used to create Attribute s which can be added to the Observation s. After the annotations are uploaded, the users must tell MEV how to interpret the columns (e.g. as a string? as a bounded float?), but once that type is specified, we can validate the annotations against that choice and subsequently add the Attribute s to the Observation s. class resource_types.table_types. BEDFile ( ) A file format that corresponds to the BED format. This is the minimal BED format, which has: chromosome start position end position Additional columns are ignored. By default, BED files do NOT contain headers and we enforce that here. Sequence-based formats class resource_types.sequence_types. SequenceResource ( ) This class is used to represent sequence-based files such as Fasta, Fastq, SAM/BAM We cannot (reasonably) locally validate the contents of these files quickly or exhaustively, so minimal validation is performed remotely class resource_types.sequence_types. FastAResource ( ) This type is for validating Fasta files, compressed or not. Fasta files are recognized using the following formats: fasta fasta.gz fa fa.gz class resource_types.sequence_types. FastQResource ( ) This resource type is for Fastq files, compressed or not. Fastq files are recognized using the following formats: fastq fastq.gz fq fq.gz class resource_types.sequence_types. AlignedSequenceResource ( ) This resource type is for SAM/BAM files. We accept both SAM and BAM files named using their canonical extensions: \".bam\" for BAM files \".sam\" for SAM files","title":"Resource types"},{"location":"resource_types/#resource-types","text":"A Resource represents some generic notion of data and its resource_type field/member is a string identifier that identifies the specific format of the data. The string identifiers map to concrete classes that implement validation methods for the Resource . When a new Resource is added (via upload or directly by an admin via the API), the validation method is called. Similarly, if a user tries to change the resource_type , it will trigger the validation process. Current resource_types fall into two broad categories: Table-based formats Sequence-based formats Table-based formats are any array-like format, such as a typical CSV file. This covers a wide variety of standard formats encountered in computational biology, including GTF annotation files and BED files. The primitive data types contained in each column are determined using Python's Pandas library, which refers to these as \"dtypes\"; for example, a column identified as int64 certainly qualifies as an integer type. If the column contains any non-integers (but all numbers), Pandas automatically converts it to a float type (e.g. float64 ) which allows us to easily validate the content of each column. Sequence-based formats are formats like FastQ, Fasta, or SAM/BAM.","title":"Resource types"},{"location":"resource_types/#table-based-resource-types","text":"class resource_types.table_types. TableResource ( ) The TableResource is the most generic form of a delimited file. Any type of data that can be represented as rows and columns. This or any of the more specific subclasses can be contained in files saved in CSV, TSV, or Excel (xls/xlsx) format. If in Excel format, the data of interest must reside in the first sheet of the workbook. Special tab-delimited files like BED or VCF files are recognized by their canonical extension (e.g. \".bed\" or \".vcf\"). Note that unless you create a \"specialized\" implementation (e.g. like for a BED file), then we assume you have features as rows and observables as columns. class resource_types.table_types. Matrix ( ) A Matrix is a delimited table-based file that has only numeric types. These types can be mixed, like floats and integers class resource_types.table_types. IntegerMatrix ( ) An IntegerMatrix further specializes the Matrix to admit only integers. class resource_types.table_types. AnnotationTable ( ) An AnnotationTable is a special type of table that will be responsible for annotating Observations/samples (e.g. adding sample names and associated attributes like experimental group or other covariates) The first column will give the sample names and the remaining columns will each individually represent different covariates associated with that sample. For example, if we received the following table: sample genotype treatment A WT Y B WT N Then this table can be used to create Attribute s which can be added to the Observation s. After the annotations are uploaded, the users must tell MEV how to interpret the columns (e.g. as a string? as a bounded float?), but once that type is specified, we can validate the annotations against that choice and subsequently add the Attribute s to the Observation s. class resource_types.table_types. BEDFile ( ) A file format that corresponds to the BED format. This is the minimal BED format, which has: chromosome start position end position Additional columns are ignored. By default, BED files do NOT contain headers and we enforce that here.","title":"Table-based resource types"},{"location":"resource_types/#sequence-based-formats","text":"class resource_types.sequence_types. SequenceResource ( ) This class is used to represent sequence-based files such as Fasta, Fastq, SAM/BAM We cannot (reasonably) locally validate the contents of these files quickly or exhaustively, so minimal validation is performed remotely class resource_types.sequence_types. FastAResource ( ) This type is for validating Fasta files, compressed or not. Fasta files are recognized using the following formats: fasta fasta.gz fa fa.gz class resource_types.sequence_types. FastQResource ( ) This resource type is for Fastq files, compressed or not. Fastq files are recognized using the following formats: fastq fastq.gz fq fq.gz class resource_types.sequence_types. AlignedSequenceResource ( ) This resource type is for SAM/BAM files. We accept both SAM and BAM files named using their canonical extensions: \".bam\" for BAM files \".sam\" for SAM files","title":"Sequence-based formats"},{"location":"resources/","text":"Resources Much of the information regarding Resource instances is provided in the auto-generated docstring below, but here we highlight some key elements of the Resource model. Namely, the kinds of operations users and admins can take to create, delete, or otherwise manipulated Resource s via the API. Resource creation Regular MEV users can only create Resource instances by uploading files, either via a direct method (upload from local machine) or by using one our cloud-based uploaders. They can't do this via the API. Admins can \"override\" and create Resource instances manually via the API. Regardless of who created the Resource , the validation process is started asynchronously. We cannot assume that the files are properly validated, even if the request was initiated by an admin. Upon creation of the Resource , it is immediately set to \"inactive\" ( is_active = False ) while we validate the particular type. Resource instances have a single owner, which is the owner who uploaded the file, or directly specified by the admin in the API request. Resource \"type\" A Resource is required to have a \"type\" (e.g. an integer matrix) which we call a resource_type . These types are restricted to a set of common file formats. Upon creation, resource_type is set to None which indicates that the Resource has not been validated. The type of the Resource can be specified when the file is uploaded or at any other time (i.e. users can change the type if they desire). Each request to change type initiates an asynchronous validation process. If the validation of the resource_type fails, we revert back to the previous successfully validated type. If the type was previously None (as with a new upload), we simply revert back to None and inform the user the validation failed. Resources and Workspaces Resource instances are initially \"unattached\" meaning they are associated with their owner, but have not been associated with any user workspaces. Admins can, however, specify a Workspace in their request to create the Resource directly via the API. When a user chooses to \"add\" a Resource to a Workspace , a new database record is created which is a copy of the original, unattached Resource with the same attributes except the unique Resource UUID. Thus, we have two database records referencing the same file. We could accomplish something similar with a many-to-one mapping of Workspace to Resource s, but this was a choice we made which could allow for resource-copying if we ever allow file-editing in the future. In that case, attaching a Resource to a Workspace could create a copy of the file such that the original Resource remains unaltered. The user can, of course, change any of the mutable members of this new Workspace -associated Resource . The changes will be independent of the original \"unattached\" Resource . Users can remove a Resource from a Workspace if it has NOT been used for any portions of the analysis. We want to retain the completeness of the analysis, so deleting files that are part of the analysis \"tree\" would create gaps. Deletion of Resources Since multiple database records can reference the same underlying file, we have a bit of custom logic for determining when we delete only the database record versus deleting the actual underlying file. Essentially, if a deletion is requested and no other Resource database records reference the same file, then we delete both the database record AND the file. In the case where there is another database record referencing that file, we only remove the database record, leaving the file. Notes related to backend implementation In general, the is_active = False flag disallows any updating of the Resource attributes via the API. All post/patch/put requests will return a 400 status. This prevents multiple requests from interfering with an ongoing background process. Users cannot change the path member. The actual storage of the files should not matter to the users so they are unable to change the path member. class api.models.resource. Resource ( *args , **kwargs ) A Resource is an abstraction of data. It represents some piece of data we are analyzing or manipulating in the course of an analysis workflow. Resource s are most often represented by flat files, but their physical storage is not important. They could be stored locally or in cloud storage accessible to MEV. Various \"types\" of Resource s implement specific constraints on the data that are important for tracking inputs and outputs of analyses. For example, if an analysis module needs to operate on a matrix of integers, we can enforce that the only Resource s available as inputs are those identified (and verified) as IntegerMatrix \"types\". Note that we store all types of Resource s in the database as a single table and maintain the notion of \"type\" by a string-field identifier. Creating specific database tables for each type of Resource would be unnecessary. By connecting the string stored in the database with a concrete implementation class we can check the type of the Resource . Resource s are not active ( is_active flag in the database) until their \"type\" has been verified. API users will submit the intended type with the request and the backend will check that. Violations are reported and the Resource remains inactive ( is_active=False ). Some additional notes: Resource s are owned by users and can be added to a Workspace . However, that is not required-- Resource s can be \"unattached\". Regular users (non-admins) can't create new Resource directly via the API. The only way they can create a Resource is indirectly by adding a new upload. When a Resource is added to a Workspace , a new copy of the database record is made. This maintains the state of the original Resource . Resource s can be made \"public\" so that others can view and import them. Once another user chooses to import the file, a copy is made and that new user has their own copy. If a Resource is later made \"private\" then any files that have been \"used\" by others cannot be recalled. Resource s can be removed from a Workspace , but only if they have not been used for any analyses/operations. Resource s cannot be transferred from one Workspace to another, but they can be copied. A change in the type of the Resource can be requested. Until the validation of that change is complete, the Resource is made private and inactive. Admins can make essentially any change to Resources , including creation. However, they must be careful to maintain the integrity of the database and the files they point to. In a request to create a Resource via the API, the resource_type field can be blank/null. The type can be inferred from the path of the resource. We can do this because only admins are allowed to create via the API and they should only generate such requests if the resource type can be inferred (i.e. admins know not to give bad requests to the API...)","title":"General info"},{"location":"resources/#resources","text":"Much of the information regarding Resource instances is provided in the auto-generated docstring below, but here we highlight some key elements of the Resource model. Namely, the kinds of operations users and admins can take to create, delete, or otherwise manipulated Resource s via the API. Resource creation Regular MEV users can only create Resource instances by uploading files, either via a direct method (upload from local machine) or by using one our cloud-based uploaders. They can't do this via the API. Admins can \"override\" and create Resource instances manually via the API. Regardless of who created the Resource , the validation process is started asynchronously. We cannot assume that the files are properly validated, even if the request was initiated by an admin. Upon creation of the Resource , it is immediately set to \"inactive\" ( is_active = False ) while we validate the particular type. Resource instances have a single owner, which is the owner who uploaded the file, or directly specified by the admin in the API request. Resource \"type\" A Resource is required to have a \"type\" (e.g. an integer matrix) which we call a resource_type . These types are restricted to a set of common file formats. Upon creation, resource_type is set to None which indicates that the Resource has not been validated. The type of the Resource can be specified when the file is uploaded or at any other time (i.e. users can change the type if they desire). Each request to change type initiates an asynchronous validation process. If the validation of the resource_type fails, we revert back to the previous successfully validated type. If the type was previously None (as with a new upload), we simply revert back to None and inform the user the validation failed. Resources and Workspaces Resource instances are initially \"unattached\" meaning they are associated with their owner, but have not been associated with any user workspaces. Admins can, however, specify a Workspace in their request to create the Resource directly via the API. When a user chooses to \"add\" a Resource to a Workspace , a new database record is created which is a copy of the original, unattached Resource with the same attributes except the unique Resource UUID. Thus, we have two database records referencing the same file. We could accomplish something similar with a many-to-one mapping of Workspace to Resource s, but this was a choice we made which could allow for resource-copying if we ever allow file-editing in the future. In that case, attaching a Resource to a Workspace could create a copy of the file such that the original Resource remains unaltered. The user can, of course, change any of the mutable members of this new Workspace -associated Resource . The changes will be independent of the original \"unattached\" Resource . Users can remove a Resource from a Workspace if it has NOT been used for any portions of the analysis. We want to retain the completeness of the analysis, so deleting files that are part of the analysis \"tree\" would create gaps. Deletion of Resources Since multiple database records can reference the same underlying file, we have a bit of custom logic for determining when we delete only the database record versus deleting the actual underlying file. Essentially, if a deletion is requested and no other Resource database records reference the same file, then we delete both the database record AND the file. In the case where there is another database record referencing that file, we only remove the database record, leaving the file. Notes related to backend implementation In general, the is_active = False flag disallows any updating of the Resource attributes via the API. All post/patch/put requests will return a 400 status. This prevents multiple requests from interfering with an ongoing background process. Users cannot change the path member. The actual storage of the files should not matter to the users so they are unable to change the path member. class api.models.resource. Resource ( *args , **kwargs ) A Resource is an abstraction of data. It represents some piece of data we are analyzing or manipulating in the course of an analysis workflow. Resource s are most often represented by flat files, but their physical storage is not important. They could be stored locally or in cloud storage accessible to MEV. Various \"types\" of Resource s implement specific constraints on the data that are important for tracking inputs and outputs of analyses. For example, if an analysis module needs to operate on a matrix of integers, we can enforce that the only Resource s available as inputs are those identified (and verified) as IntegerMatrix \"types\". Note that we store all types of Resource s in the database as a single table and maintain the notion of \"type\" by a string-field identifier. Creating specific database tables for each type of Resource would be unnecessary. By connecting the string stored in the database with a concrete implementation class we can check the type of the Resource . Resource s are not active ( is_active flag in the database) until their \"type\" has been verified. API users will submit the intended type with the request and the backend will check that. Violations are reported and the Resource remains inactive ( is_active=False ). Some additional notes: Resource s are owned by users and can be added to a Workspace . However, that is not required-- Resource s can be \"unattached\". Regular users (non-admins) can't create new Resource directly via the API. The only way they can create a Resource is indirectly by adding a new upload. When a Resource is added to a Workspace , a new copy of the database record is made. This maintains the state of the original Resource . Resource s can be made \"public\" so that others can view and import them. Once another user chooses to import the file, a copy is made and that new user has their own copy. If a Resource is later made \"private\" then any files that have been \"used\" by others cannot be recalled. Resource s can be removed from a Workspace , but only if they have not been used for any analyses/operations. Resource s cannot be transferred from one Workspace to another, but they can be copied. A change in the type of the Resource can be requested. Until the validation of that change is complete, the Resource is made private and inactive. Admins can make essentially any change to Resources , including creation. However, they must be careful to maintain the integrity of the database and the files they point to. In a request to create a Resource via the API, the resource_type field can be blank/null. The type can be inferred from the path of the resource. We can do this because only admins are allowed to create via the API and they should only generate such requests if the resource type can be inferred (i.e. admins know not to give bad requests to the API...)","title":"Resources"},{"location":"setup_configuration/","text":"Configuration options and parameters As described in the installation section, the WebMEV API depends on environment variables passed to the container using the --env-file argument. Most variables defined in env_vars.template.txt should be appropriately commented, but we provide some additional instructions or commentary below. Email backends ( EMAIL_BACKEND_CHOICE ) Various operations like user registration and password changes require us to send users emails with encoded tokens to verify their email address and permit changes to their account. Accordingly, WebMEV needs the ability to send emails. We allow customization of this email backend through the EMAIL_BACKEND_CHOICE variable in env_vars.template.txt . Certain cloud providers, such as GCP, place restrictions on outgoing emails to prevent abuse. Per their documentation ( https://cloud.google.com/compute/docs/tutorials/sending-mail ), GCP recommends to use a third-party service such as SendGrid. If you wish to use these, you will have to implement your own email backend per the interface described at the Django project: https://docs.djangoproject.com/en/3.0/topics/email/#defining-a-custom-email-backend By default, we provide the following email backends: Console ( EMAIL_BACKEND_CHOICE=CONSOLE ) Note: This backend is for development purposes only-- no emails are actually sent! If EMAIL_BACKEND_CHOICE is not set, WebMEV defaults to using Django's \"console\" backend, which simply prints the emails to stdout. This is fine for development purposes where the tokens can be copy/pasted for live-testing, but is obviously not suitable for a production environment. Gmail ( EMAIL_BACKEND_CHOICE=GMAIL ) Alternatively, one can use the Gmail API to send emails from their personal or institution google account. In addition to setting EMAIL_BACKEND_CHOICE=GMAIL , you will need to set the following additional variables: GMAIL_ACCESS_TOKEN= GMAIL_REFRESH_TOKEN= GMAIL_CLIENT_ID= GMAIL_CLIENT_SECRET= These variables are obtained after you have created appropriate credentials and performed an Oauth2-based exchange, which we describe. Steps: Choose a Gmail account (or create one anew) from which you wish to send email notifications. On your own machine (or wherever you can login to your Google account), go to the Google developers console ( https://console.developers.google.com or https://console.cloud.google.com ) and head to \"APIs & Services\" and \"Dashboard\". Click on \"Enable APIs and Services\", search for \"Gmail\", and enable the Gmail API. Once that is enabled, go to the \"Credentials\" section under \"APIs and Services\". Just as above, we will create a set of OAuth credentials. Click on the \"Create credentials\" button and choose \"OAuth Client ID\". Choose \"Other\" from the options and give these credentials a name. Once the credentials are created, download the JSON-format file when it prompts. Using that credential file, run the helpers/exchange_gmail_credentials.py script like: python3 helpers/exchange_gmail_credentials.py -i <original_creds_path> -o <final_creds_path> (Note that this script can be run from within the WebMEV Docker container, as it contains the appropriate Python packages. If you are not using the application container, you can run the script as long as the google-auth-oauthlib library is installed-- https://pypi.org/project/google-auth-oauthlib/ ) The script will ask you to copy a link into your browser, which you can do on any machine where you can authenticate with Google. That URL will ask you to choose/log-in to the Gmail account you will be using to send emails. Finally, if successfully authenticated, it will provide you with a \"code\" which you will copy into your terminal. Once complete, the script will write a new JSON-format file at the location specified with the -o argument. Using the values in that final JSON file, copy/paste those into the file of environment variables you will be submitting to the WebMEV container upon startup. Be careful with these credentials as they give full access to the Gmail account in question!! Storage backends ( RESOURCE_STORAGE_BACKEND ) Storage of user files can be either local (on the MEV server) or in some remote filesystem (such as in a Google storage bucket). To abstract this, we have storage \"backends\" that control the behavior for each storage choice. Options for storage backends can be found in the api/storage_backends folder. To use a particular backend, we supply the \"dotted\" path for the class that implements the storage interface. For example, to use the Google bucket storage backend, we would use RESOURCE_STORAGE_BACKEND=api.storage_backends.google_cloud.GoogleBucketStorage since the GoogleBucketStorage class is located in api/storage_backends/google_cloud.py . Note that each storage backend may require additional environment variables to be set. We enforce this by attempting an initial import of the storage backend class. By convention, any configuration parameters required should at the \"top-level\" of the Python module/file. This way, when we attempt the import while starting the application, any missing configuration variables will raise an exception. This (hopefully) prevents errors in runtime due to incomplete/invalid configuration.","title":"Configuration"},{"location":"setup_configuration/#configuration-options-and-parameters","text":"As described in the installation section, the WebMEV API depends on environment variables passed to the container using the --env-file argument. Most variables defined in env_vars.template.txt should be appropriately commented, but we provide some additional instructions or commentary below.","title":"Configuration options and parameters"},{"location":"setup_configuration/#email-backends-email_backend_choice","text":"Various operations like user registration and password changes require us to send users emails with encoded tokens to verify their email address and permit changes to their account. Accordingly, WebMEV needs the ability to send emails. We allow customization of this email backend through the EMAIL_BACKEND_CHOICE variable in env_vars.template.txt . Certain cloud providers, such as GCP, place restrictions on outgoing emails to prevent abuse. Per their documentation ( https://cloud.google.com/compute/docs/tutorials/sending-mail ), GCP recommends to use a third-party service such as SendGrid. If you wish to use these, you will have to implement your own email backend per the interface described at the Django project: https://docs.djangoproject.com/en/3.0/topics/email/#defining-a-custom-email-backend By default, we provide the following email backends: Console ( EMAIL_BACKEND_CHOICE=CONSOLE ) Note: This backend is for development purposes only-- no emails are actually sent! If EMAIL_BACKEND_CHOICE is not set, WebMEV defaults to using Django's \"console\" backend, which simply prints the emails to stdout. This is fine for development purposes where the tokens can be copy/pasted for live-testing, but is obviously not suitable for a production environment. Gmail ( EMAIL_BACKEND_CHOICE=GMAIL ) Alternatively, one can use the Gmail API to send emails from their personal or institution google account. In addition to setting EMAIL_BACKEND_CHOICE=GMAIL , you will need to set the following additional variables: GMAIL_ACCESS_TOKEN= GMAIL_REFRESH_TOKEN= GMAIL_CLIENT_ID= GMAIL_CLIENT_SECRET= These variables are obtained after you have created appropriate credentials and performed an Oauth2-based exchange, which we describe. Steps: Choose a Gmail account (or create one anew) from which you wish to send email notifications. On your own machine (or wherever you can login to your Google account), go to the Google developers console ( https://console.developers.google.com or https://console.cloud.google.com ) and head to \"APIs & Services\" and \"Dashboard\". Click on \"Enable APIs and Services\", search for \"Gmail\", and enable the Gmail API. Once that is enabled, go to the \"Credentials\" section under \"APIs and Services\". Just as above, we will create a set of OAuth credentials. Click on the \"Create credentials\" button and choose \"OAuth Client ID\". Choose \"Other\" from the options and give these credentials a name. Once the credentials are created, download the JSON-format file when it prompts. Using that credential file, run the helpers/exchange_gmail_credentials.py script like: python3 helpers/exchange_gmail_credentials.py -i <original_creds_path> -o <final_creds_path> (Note that this script can be run from within the WebMEV Docker container, as it contains the appropriate Python packages. If you are not using the application container, you can run the script as long as the google-auth-oauthlib library is installed-- https://pypi.org/project/google-auth-oauthlib/ ) The script will ask you to copy a link into your browser, which you can do on any machine where you can authenticate with Google. That URL will ask you to choose/log-in to the Gmail account you will be using to send emails. Finally, if successfully authenticated, it will provide you with a \"code\" which you will copy into your terminal. Once complete, the script will write a new JSON-format file at the location specified with the -o argument. Using the values in that final JSON file, copy/paste those into the file of environment variables you will be submitting to the WebMEV container upon startup. Be careful with these credentials as they give full access to the Gmail account in question!!","title":"Email backends (EMAIL_BACKEND_CHOICE)"},{"location":"setup_configuration/#storage-backends-resource_storage_backend","text":"Storage of user files can be either local (on the MEV server) or in some remote filesystem (such as in a Google storage bucket). To abstract this, we have storage \"backends\" that control the behavior for each storage choice. Options for storage backends can be found in the api/storage_backends folder. To use a particular backend, we supply the \"dotted\" path for the class that implements the storage interface. For example, to use the Google bucket storage backend, we would use RESOURCE_STORAGE_BACKEND=api.storage_backends.google_cloud.GoogleBucketStorage since the GoogleBucketStorage class is located in api/storage_backends/google_cloud.py . Note that each storage backend may require additional environment variables to be set. We enforce this by attempting an initial import of the storage backend class. By convention, any configuration parameters required should at the \"top-level\" of the Python module/file. This way, when we attempt the import while starting the application, any missing configuration variables will raise an exception. This (hopefully) prevents errors in runtime due to incomplete/invalid configuration.","title":"Storage backends (RESOURCE_STORAGE_BACKEND)"},{"location":"workspaces/","text":"Workspaces class api.models.workspace. Workspace ( *args , **kwargs ) A Workspace is a way to logically group the files and and analyses that are part of a user's work. Users can have multiple Workspace s to separate distinct analyses. Data, files, and analyses are grouped under a Workspace such that all information related to the analyses, including analysis history, is captured the Workspace .","title":"Workspaces"},{"location":"workspaces/#workspaces","text":"class api.models.workspace. Workspace ( *args , **kwargs ) A Workspace is a way to logically group the files and and analyses that are part of a user's work. Users can have multiple Workspace s to separate distinct analyses. Data, files, and analyses are grouped under a Workspace such that all information related to the analyses, including analysis history, is captured the Workspace .","title":"Workspaces"}]}